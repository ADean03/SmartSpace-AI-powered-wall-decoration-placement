{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOB3SFgtCiQHgTjc+1qJmVQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ADean03/SmartSpace-AI-powered-wall-decoration-placement/blob/main/wallseg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7NmqSFNG3c4",
        "outputId": "4fdde310-10c5-48d7-944e-ae4125f264a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ADean03/WallSegmentation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXcGq_ckILeB",
        "outputId": "44791161-1de8-4e4f-fe46-f9ed0a7f07df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'WallSegmentation'...\n",
            "remote: Enumerating objects: 356, done.\u001b[K\n",
            "remote: Counting objects: 100% (85/85), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 356 (delta 32), reused 45 (delta 14), pack-reused 271 (from 1)\u001b[K\n",
            "Receiving objects: 100% (356/356), 84.54 MiB | 18.93 MiB/s, done.\n",
            "Resolving deltas: 100% (159/159), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd WallSegmentation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPZSbGwTIRyh",
        "outputId": "4997822d-71b0-475c-f404-41c4ddc97b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/WallSegmentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFBlHAPUIc8q",
        "outputId": "e80fc232-b65f-4f16-c9b7-1a16de4b2716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mckpt\u001b[0m/     LICENSE         README.md              \u001b[01;34msrc\u001b[0m/             \u001b[01;34mutils\u001b[0m/\n",
            "\u001b[01;34mconfigs\u001b[0m/  \u001b[01;34mmodels\u001b[0m/         \u001b[01;34mreadme_supplementary\u001b[0m/  testing.ipynb\n",
            "\u001b[01;34mdata\u001b[0m/     \u001b[01;34mmodel_weights\u001b[0m/  requirements.txt       train_script.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIC-0RKPIrvu",
        "outputId": "18eb9be8-7db4-4247-f19b-bfb46d39e044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 2)) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 3)) (4.67.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 4)) (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 2)) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->-r requirements.txt (line 2)) (11.3.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.9)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 4)) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 4)) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 4)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->-r requirements.txt (line 4)) (3.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 4)) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYqiNeOrGGcS"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "from models.models import SegmentationModule, build_encoder, build_decoder\n",
        "from src.eval import segment_image\n",
        "from utils.constants import DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "file_path = '/content/drive/My Drive/SmartSpace'\n",
        "os.listdir('/content/drive/MyDrive/SmartSpace')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq0kS52_IvKo",
        "outputId": "ea56a14e-26f5-4dc9-f3f3-0eeb9a0f0ec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['wallsegWeights', 'homeobjects-3K', 'visualizations', 'wallseg.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing getting segmented image"
      ],
      "metadata": {
        "id": "Joy04UpULIjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/SmartSpace/homeobjects-3K/images/train/living_room_1p (133).jpg'\n",
        "print(os.path.exists(file_path))\n",
        "#os.listdir('/content/drive/My Drive/SmartSpace/homeobjects-3K/images/train/living_room_1p (133).jpg')\n",
        "#os.listdir('/content/drive/MyDrive/SmartSpace/wallsegWeights')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSDtVkTCLSsw",
        "outputId": "ec6dad8f-9d28-4a00-99bf-0a48d41f1ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_image = file_path\n",
        "\n",
        "# Model weights (encoder and decoder)\n",
        "weights_encoder = '/content/drive/MyDrive/SmartSpace/wallsegWeights/best_encoder_epoch_19.pth'\n",
        "weights_decoder = '/content/drive/MyDrive/SmartSpace/wallsegWeights/best_decoder_epoch_19.pth'"
      ],
      "metadata": {
        "id": "tbtlA0RIM_oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net_encoder = build_encoder(weights_encoder)\n",
        "net_decoder = build_decoder(weights_decoder)\n",
        "\n",
        "segmentation_module = SegmentationModule(net_encoder, net_decoder)\n",
        "segmentation_module = segmentation_module.to(DEVICE).eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnfkOxlSOS1r",
        "outputId": "53728254-6261-4bff-e2c4-4888dd872bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building encoder: resnet50-dilated\n",
            "Loading weights for net_encoder\n",
            "Loading weights for net_decoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segmentation_mask = segment_image(segmentation_module, path_image)"
      ],
      "metadata": {
        "id": "IwYAxLKxOaMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(segmentation_mask)\n",
        "plt.axis('off')        # hides the x/y axis\n",
        "plt.tight_layout()     # removes padding\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "XDgL_JsEQHNh",
        "outputId": "852c82a9-38f4-489a-c5a1-92f6389b93f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The image above was testing generating a binary wall/not mask. Ultimately this approach was not used as the not-wall mask has the painting/picture frames clearly outlined and while could theoritically could be removed based off existing bounding box data, simpiler approach would be to just predict based off bounding boxes"
      ],
      "metadata": {
        "id": "A4t580Vn6Mdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(segmentation_mask))\n",
        "print(segmentation_mask.shape)\n",
        "print(segmentation_mask.min(), segmentation_mask.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iduzatZKSS6V",
        "outputId": "e0240c11-c8bb-4f97-d24a-03b8806d3bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(853, 1280)\n",
            "0 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "label_dir = '/content/drive/My Drive/SmartSpace/homeobjects-3K/labels/train_input'\n",
        "\n",
        "train_paintings_per_file = []  # list to store how many paintings are in each file\n",
        "\n",
        "for label_file in os.listdir(label_dir):\n",
        "    if not label_file.endswith('.txt'):\n",
        "        continue\n",
        "    label_path = os.path.join(label_dir, label_file)\n",
        "\n",
        "    with open(label_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        count_paintings = 0\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) == 0:\n",
        "                continue\n",
        "            class_id = int(parts[0])\n",
        "            if class_id == 11:\n",
        "                count_paintings += 1\n",
        "\n",
        "        if count_paintings > 0:\n",
        "            train_paintings_per_file.append(count_paintings)\n",
        "\n",
        "train_num_files_with_paintings = len(train_paintings_per_file)\n",
        "train_total_paintings = sum(train_paintings_per_file)\n",
        "train_avg_paintings_per_file = train_total_paintings / train_num_files_with_paintings if train_num_files_with_paintings > 0 else 0\n",
        "\n",
        "print(f\"Found {train_num_files_with_paintings} images with paintings\")\n",
        "print(f\"Average number of paintings per such image: {train_avg_paintings_per_file:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "-RdMSOMNVJVX",
        "outputId": "8c44b876-c606-45a9-b47e-1309a4f809b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3459368415.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mcount_paintings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "label_dir = '/content/drive/My Drive/SmartSpace/homeobjects-3K/labels/val_input'\n",
        "\n",
        "val_paintings_per_file = []  # list to store how many paintings are in each file\n",
        "\n",
        "for label_file in os.listdir(label_dir):\n",
        "    if not label_file.endswith('.txt'):\n",
        "        continue\n",
        "    label_path = os.path.join(label_dir, label_file)\n",
        "\n",
        "    with open(label_path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        count_paintings = 0\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            parts = line.split()\n",
        "            if len(parts) == 0:\n",
        "                continue\n",
        "            class_id = int(parts[0])\n",
        "            if class_id == 11:\n",
        "                count_paintings += 1\n",
        "\n",
        "        if count_paintings > 0:\n",
        "            val_paintings_per_file.append(count_paintings)\n",
        "\n",
        "val_num_files_with_paintings = len(val_paintings_per_file)\n",
        "val_total_paintings = sum(val_paintings_per_file)\n",
        "val_avg_paintings_per_file = val_total_paintings / val_num_files_with_paintings if val_num_files_with_paintings > 0 else 0\n",
        "\n",
        "print(f\"Found {val_num_files_with_paintings} images with paintings\")\n",
        "print(f\"Average number of paintings per such image: {val_avg_paintings_per_file:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvgJ97Y0UIRk",
        "outputId": "7fa182ae-40a6-4a5e-a6a6-58881e8d60be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 images with paintings\n",
            "Average number of paintings per such image: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Paths\n",
        "base_dir = '/content/drive/My Drive/SmartSpace/homeobjects-3K/labels'\n",
        "val_input_dir = os.path.join(base_dir, 'val_input')\n",
        "val_input_aug_dir = os.path.join(base_dir, 'val_input_aug')\n",
        "\n",
        "# Create the output folder if it doesn’t exist\n",
        "os.makedirs(val_input_aug_dir, exist_ok=True)\n",
        "\n",
        "for label_file in os.listdir(val_input_dir):\n",
        "    if not label_file.endswith('.txt'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(val_input_dir, label_file)\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = [l.strip() for l in f.readlines() if l.strip()]\n",
        "\n",
        "    # Separate class 11 and other objects\n",
        "    class11_lines = [l for l in lines if l.split()[0] == '11']\n",
        "    other_lines = [l for l in lines if l.split()[0] != '11']\n",
        "\n",
        "    # 1️⃣ Overwrite the original file (in val_input) with all class 11s removed\n",
        "    with open(file_path, 'w') as f:\n",
        "        for line in other_lines:\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "    # 2️⃣ If more than 2 paintings, create an augmented copy in val_input_aug\n",
        "    if len(class11_lines) > 2:\n",
        "        num_to_keep = random.randint(1, len(class11_lines) - 1)\n",
        "        kept_class11 = random.sample(class11_lines, num_to_keep)\n",
        "\n",
        "        new_lines = other_lines + kept_class11\n",
        "        random.shuffle(new_lines)  # optional\n",
        "\n",
        "        new_filename = label_file.replace('.txt', '_aug.txt')\n",
        "        new_file_path = os.path.join(val_input_aug_dir, new_filename)\n",
        "\n",
        "        with open(new_file_path, 'w') as f:\n",
        "            for line in new_lines:\n",
        "                f.write(line + '\\n')\n",
        "\n",
        "        print(f\"Created augmented file: {new_filename} (kept {num_to_keep} of {len(class11_lines)})\")\n",
        "\n",
        "print(\"✅ Finished cleaning val_input and creating val_input_aug.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqTn1PnjIRRv",
        "outputId": "b567c302-eec1-45b2-edcc-ba6448372e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created augmented file: living_room_1p (143)_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1p (365)_aug.txt (kept 3 of 7)\n",
            "Created augmented file: living_room_1p (218)_aug.txt (kept 5 of 8)\n",
            "Created augmented file: living_room_1p (169)_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1p (260)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (353)_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1p (131)_aug.txt (kept 5 of 6)\n",
            "Created augmented file: living_room_1p (164)_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_1p (160)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (272)_aug.txt (kept 1 of 5)\n",
            "Created augmented file: living_room_1p (35)_aug.txt (kept 1 of 7)\n",
            "Created augmented file: living_room_1p (412)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (3)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (374)_aug.txt (kept 2 of 6)\n",
            "Created augmented file: living_room_1p (101)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (439)_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1p (314)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (209)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_180_aug.txt (kept 6 of 13)\n",
            "Created augmented file: living_room_107_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_115_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1012_aug.txt (kept 6 of 7)\n",
            "Created augmented file: living_room_1024_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_160_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1082_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1036_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1072_aug.txt (kept 4 of 10)\n",
            "Created augmented file: living_room_987_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_971_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_975_aug.txt (kept 14 of 19)\n",
            "Created augmented file: living_room_896_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_892_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_805_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_954_aug.txt (kept 15 of 17)\n",
            "Created augmented file: living_room_82_aug.txt (kept 8 of 9)\n",
            "Created augmented file: living_room_958_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_801_aug.txt (kept 6 of 7)\n",
            "Created augmented file: living_room_760_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_768_aug.txt (kept 8 of 9)\n",
            "Created augmented file: living_room_744_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_728_aug.txt (kept 3 of 7)\n",
            "Created augmented file: living_room_908_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_74_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_627_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_598_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_62_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_570_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_46_aug.txt (kept 3 of 7)\n",
            "Created augmented file: living_room_542_aug.txt (kept 4 of 6)\n",
            "Created augmented file: living_room_50_aug.txt (kept 3 of 6)\n",
            "Created augmented file: living_room_404_aug.txt (kept 4 of 6)\n",
            "Created augmented file: living_room_408_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_550_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_323_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_311_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_266_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_392_aug.txt (kept 8 of 10)\n",
            "Created augmented file: living_room_1p (97)_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_278_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_270_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_42_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1p (93)_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_282_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_262_aug.txt (kept 5 of 6)\n",
            "Created augmented file: living_room_1p (64)_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1p (469)_aug.txt (kept 6 of 7)\n",
            "Created augmented file: living_room_1p (501)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (597)_aug.txt (kept 1 of 5)\n",
            "Created augmented file: living_room_1p (52)_aug.txt (kept 2 of 6)\n",
            "Created augmented file: living_room_1p (546)_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1p (56)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (514)_aug.txt (kept 2 of 8)\n",
            "Created augmented file: living_room_1p (584)_aug.txt (kept 3 of 4)\n",
            "✅ Finished cleaning val_input and creating val_input_aug.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LeC0SOTuPivs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "base_dir = '/content/drive/My Drive/SmartSpace/homeobjects-3K/labels'\n",
        "train_input_dir = os.path.join(base_dir, 'train_input')\n",
        "train_input_aug_dir = os.path.join(base_dir, 'train_input_aug')\n",
        "\n",
        "# Create the output folder if it doesn’t exist\n",
        "os.makedirs(train_input_aug_dir, exist_ok=True)\n",
        "\n",
        "for label_file in os.listdir(train_input_dir):\n",
        "    if not label_file.endswith('.txt'):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(train_input_dir, label_file)\n",
        "\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = [l.strip() for l in f.readlines() if l.strip()]\n",
        "\n",
        "    # Separate class 11 and other objects\n",
        "    class11_lines = [l for l in lines if l.split()[0] == '11']\n",
        "    other_lines = [l for l in lines if l.split()[0] != '11']\n",
        "\n",
        "    # 1️⃣ Overwrite the original file (in train_input) with all class 11s removed\n",
        "    with open(file_path, 'w') as f:\n",
        "        for line in other_lines:\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "    # 2️⃣ If more than 2 paintings, create an augmented copy in train_input_aug\n",
        "    if len(class11_lines) > 2:\n",
        "        num_to_keep = random.randint(1, len(class11_lines) - 1)\n",
        "        kept_class11 = random.sample(class11_lines, num_to_keep)\n",
        "\n",
        "        new_lines = other_lines + kept_class11\n",
        "        random.shuffle(new_lines)  # optional\n",
        "\n",
        "        new_filename = label_file.replace('.txt', '_aug.txt')\n",
        "        new_file_path = os.path.join(train_input_aug_dir, new_filename)\n",
        "\n",
        "        with open(new_file_path, 'w') as f:\n",
        "            for line in new_lines:\n",
        "                f.write(line + '\\n')\n",
        "\n",
        "        print(f\"Created augmented file: {new_filename} (kept {num_to_keep} of {len(class11_lines)})\")\n",
        "\n",
        "print(\"✅ Finished cleaning train_input and creating train_input_aug.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46234674-d1c6-423c-df09-fd5a5e35b966",
        "id": "r_Zn6ymRPn3h"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created augmented file: living_room_1320_aug.txt (kept 7 of 8)\n",
            "Created augmented file: living_room_1237_aug.txt (kept 4 of 11)\n",
            "Created augmented file: living_room_13_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_133_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_1303_aug.txt (kept 6 of 13)\n",
            "Created augmented file: living_room_1302_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1291_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_134_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1256_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_1257_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1228_aug.txt (kept 6 of 7)\n",
            "Created augmented file: living_room_1211_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1214_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_121_aug.txt (kept 5 of 7)\n",
            "Created augmented file: living_room_1208_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1192_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_1204_aug.txt (kept 2 of 6)\n",
            "Created augmented file: living_room_1216_aug.txt (kept 7 of 10)\n",
            "Created augmented file: living_room_1235_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1149_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1155_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_1132_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1166_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1138_aug.txt (kept 8 of 9)\n",
            "Created augmented file: living_room_1158_aug.txt (kept 17 of 22)\n",
            "Created augmented file: living_room_1167_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1151_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1152_aug.txt (kept 12 of 14)\n",
            "Created augmented file: living_room_1128_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1109_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_1130_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_106_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1159_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1056_aug.txt (kept 8 of 9)\n",
            "Created augmented file: living_room_1102_aug.txt (kept 2 of 7)\n",
            "Created augmented file: living_room_1137_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1143_aug.txt (kept 4 of 19)\n",
            "Created augmented file: living_room_114_aug.txt (kept 8 of 10)\n",
            "Created augmented file: living_room_1157_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1083_aug.txt (kept 7 of 12)\n",
            "Created augmented file: living_room_1096_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1108_aug.txt (kept 3 of 7)\n",
            "Created augmented file: living_room_1111_aug.txt (kept 4 of 6)\n",
            "Created augmented file: living_room_109_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1062_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_108_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1090_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1030_aug.txt (kept 3 of 21)\n",
            "Created augmented file: living_room_1005_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_101_aug.txt (kept 2 of 8)\n",
            "Created augmented file: living_room_1011_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1010_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1626_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1659_aug.txt (kept 4 of 6)\n",
            "Created augmented file: living_room_1622_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_159_aug.txt (kept 3 of 7)\n",
            "Created augmented file: living_room_1580_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1630_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1562_aug.txt (kept 1 of 5)\n",
            "Created augmented file: living_room_1603_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1602_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1608_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1593_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_1557_aug.txt (kept 3 of 8)\n",
            "Created augmented file: living_room_1507_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1512_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_1474_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1503_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_1539_aug.txt (kept 8 of 10)\n",
            "Created augmented file: living_room_154_aug.txt (kept 1 of 5)\n",
            "Created augmented file: living_room_1447_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1470_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1448_aug.txt (kept 4 of 9)\n",
            "Created augmented file: living_room_1457_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1465_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_149_aug.txt (kept 5 of 6)\n",
            "Created augmented file: living_room_1410_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1468_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1390_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1391_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_1435_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1421_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1400_aug.txt (kept 6 of 8)\n",
            "Created augmented file: living_room_1395_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1428_aug.txt (kept 1 of 12)\n",
            "Created augmented file: living_room_1344_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1380_aug.txt (kept 6 of 8)\n",
            "Created augmented file: living_room_1379_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1366_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1369_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1405_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_136_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1358_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1382_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1342_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1p (343)_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1p (44)_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1p (418)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (445)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (41)_aug.txt (kept 3 of 13)\n",
            "Created augmented file: living_room_1p (40)_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1p (358)_aug.txt (kept 3 of 7)\n",
            "Created augmented file: living_room_1p (45)_aug.txt (kept 1 of 5)\n",
            "Created augmented file: living_room_1p (421)_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_1p (359)_aug.txt (kept 5 of 6)\n",
            "Created augmented file: living_room_1p (352)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (402)_aug.txt (kept 5 of 8)\n",
            "Created augmented file: living_room_1p (355)_aug.txt (kept 4 of 12)\n",
            "Created augmented file: living_room_1p (424)_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1p (461)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (446)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (398)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (375)_aug.txt (kept 1 of 6)\n",
            "Created augmented file: living_room_1p (219)_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_1p (31)_aug.txt (kept 4 of 11)\n",
            "Created augmented file: living_room_1p (334)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (312)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (202)_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_1p (317)_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1p (342)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (329)_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1p (332)_aug.txt (kept 7 of 11)\n",
            "Created augmented file: living_room_1p (257)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (267)_aug.txt (kept 2 of 7)\n",
            "Created augmented file: living_room_1p (281)_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1p (299)_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_1p (255)_aug.txt (kept 3 of 7)\n",
            "Created augmented file: living_room_1p (186)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (195)_aug.txt (kept 11 of 15)\n",
            "Created augmented file: living_room_1p (300)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (241)_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_1p (278)_aug.txt (kept 10 of 15)\n",
            "Created augmented file: living_room_1p (176)_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1p (109)_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1p (132)_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_1p (244)_aug.txt (kept 6 of 7)\n",
            "Created augmented file: living_room_1p (180)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (17)_aug.txt (kept 1 of 6)\n",
            "Created augmented file: living_room_1p (141)_aug.txt (kept 4 of 7)\n",
            "Created augmented file: living_room_1p (210)_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1p (194)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (220)_aug.txt (kept 1 of 7)\n",
            "Created augmented file: living_room_1p (262)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1963_aug.txt (kept 3 of 6)\n",
            "Created augmented file: living_room_1p (136)_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_197_aug.txt (kept 1 of 6)\n",
            "Created augmented file: living_room_1p (1)_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1971_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_198_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1960_aug.txt (kept 4 of 7)\n",
            "Created augmented file: living_room_1953_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (161)_aug.txt (kept 1 of 6)\n",
            "Created augmented file: living_room_1p (133)_aug.txt (kept 7 of 8)\n",
            "Created augmented file: living_room_1958_aug.txt (kept 8 of 11)\n",
            "Created augmented file: living_room_1921_aug.txt (kept 3 of 6)\n",
            "Created augmented file: living_room_1911_aug.txt (kept 3 of 6)\n",
            "Created augmented file: living_room_191_aug.txt (kept 4 of 6)\n",
            "Created augmented file: living_room_1897_aug.txt (kept 2 of 6)\n",
            "Created augmented file: living_room_1948_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_1903_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1885_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_185_aug.txt (kept 3 of 6)\n",
            "Created augmented file: living_room_1783_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_183_aug.txt (kept 1 of 7)\n",
            "Created augmented file: living_room_1804_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1809_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_186_aug.txt (kept 2 of 9)\n",
            "Created augmented file: living_room_1806_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_177_aug.txt (kept 2 of 6)\n",
            "Created augmented file: living_room_1802_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1764_aug.txt (kept 1 of 6)\n",
            "Created augmented file: living_room_1827_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1788_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_1782_aug.txt (kept 3 of 6)\n",
            "Created augmented file: living_room_1719_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1721_aug.txt (kept 2 of 16)\n",
            "Created augmented file: living_room_1717_aug.txt (kept 2 of 6)\n",
            "Created augmented file: living_room_1735_aug.txt (kept 3 of 10)\n",
            "Created augmented file: living_room_1700_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_1677_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_1688_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_293_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_317_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (539)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_284_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1p (547)_aug.txt (kept 1 of 5)\n",
            "Created augmented file: living_room_271_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_353_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_316_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_385_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_302_aug.txt (kept 3 of 7)\n",
            "Created augmented file: living_room_352_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_268_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_31_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_32_aug.txt (kept 8 of 12)\n",
            "Created augmented file: living_room_236_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_354_aug.txt (kept 6 of 8)\n",
            "Created augmented file: living_room_232_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (91)_aug.txt (kept 4 of 7)\n",
            "Created augmented file: living_room_28_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_337_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_251_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_225_aug.txt (kept 2 of 6)\n",
            "Created augmented file: living_room_1p (59)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_285_aug.txt (kept 1 of 5)\n",
            "Created augmented file: living_room_368_aug.txt (kept 3 of 15)\n",
            "Created augmented file: living_room_273_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_29_aug.txt (kept 4 of 6)\n",
            "Created augmented file: living_room_252_aug.txt (kept 5 of 8)\n",
            "Created augmented file: living_room_1p (73)_aug.txt (kept 2 of 6)\n",
            "Created augmented file: living_room_1p (526)_aug.txt (kept 4 of 6)\n",
            "Created augmented file: living_room_1p (585)_aug.txt (kept 2 of 6)\n",
            "Created augmented file: living_room_1p (86)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (517)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_1p (506)_aug.txt (kept 3 of 8)\n",
            "Created augmented file: living_room_1p (507)_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_1p (530)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (61)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (84)_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_1p (590)_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_1p (516)_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_1p (555)_aug.txt (kept 1 of 6)\n",
            "Created augmented file: living_room_1p (57)_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_1p (499)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_1p (87)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_84_aug.txt (kept 3 of 12)\n",
            "Created augmented file: living_room_606_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_665_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_676_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_802_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_705_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_672_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_616_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_572_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_656_aug.txt (kept 1 of 8)\n",
            "Created augmented file: living_room_80_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_604_aug.txt (kept 5 of 6)\n",
            "Created augmented file: living_room_695_aug.txt (kept 1 of 5)\n",
            "Created augmented file: living_room_71_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_516_aug.txt (kept 4 of 6)\n",
            "Created augmented file: living_room_646_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_798_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_491_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_680_aug.txt (kept 1 of 6)\n",
            "Created augmented file: living_room_664_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_762_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_697_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_65_aug.txt (kept 6 of 11)\n",
            "Created augmented file: living_room_499_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_561_aug.txt (kept 3 of 5)\n",
            "Created augmented file: living_room_759_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_507_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_650_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_811_aug.txt (kept 21 of 22)\n",
            "Created augmented file: living_room_709_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_820_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_59_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_608_aug.txt (kept 5 of 9)\n",
            "Created augmented file: living_room_535_aug.txt (kept 2 of 7)\n",
            "Created augmented file: living_room_549_aug.txt (kept 3 of 6)\n",
            "Created augmented file: living_room_430_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_49_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_406_aug.txt (kept 1 of 4)\n",
            "Created augmented file: living_room_418_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_520_aug.txt (kept 2 of 4)\n",
            "Created augmented file: living_room_427_aug.txt (kept 4 of 16)\n",
            "Created augmented file: living_room_443_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_525_aug.txt (kept 3 of 6)\n",
            "Created augmented file: living_room_464_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_423_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_477_aug.txt (kept 3 of 9)\n",
            "Created augmented file: living_room_485_aug.txt (kept 1 of 5)\n",
            "Created augmented file: living_room_529_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_402_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_435_aug.txt (kept 1 of 4)\n",
            "Created augmented file: s (349)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: s (233)_aug.txt (kept 5 of 9)\n",
            "Created augmented file: s (385)_aug.txt (kept 3 of 5)\n",
            "Created augmented file: s (238)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: s (320)_aug.txt (kept 1 of 4)\n",
            "Created augmented file: s (350)_aug.txt (kept 6 of 8)\n",
            "Created augmented file: s (324)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: s (327)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: s (247)_aug.txt (kept 4 of 5)\n",
            "Created augmented file: s (27)_aug.txt (kept 2 of 4)\n",
            "Created augmented file: s (325)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: s (159)_aug.txt (kept 2 of 6)\n",
            "Created augmented file: s (70)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: s (323)_aug.txt (kept 3 of 4)\n",
            "Created augmented file: s (2)_aug.txt (kept 3 of 5)\n",
            "Created augmented file: s (295)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: s (237)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: s (294)_aug.txt (kept 6 of 13)\n",
            "Created augmented file: s (360)_aug.txt (kept 1 of 4)\n",
            "Created augmented file: s (5)_aug.txt (kept 1 of 3)\n",
            "Created augmented file: s (19)_aug.txt (kept 2 of 5)\n",
            "Created augmented file: living_room_89_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_849_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_885_aug.txt (kept 3 of 6)\n",
            "Created augmented file: living_room_925_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_95_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_9_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_935_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_957_aug.txt (kept 1 of 6)\n",
            "Created augmented file: living_room_891_aug.txt (kept 5 of 7)\n",
            "Created augmented file: s (135)_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_871_aug.txt (kept 1 of 3)\n",
            "Created augmented file: living_room_949_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_886_aug.txt (kept 4 of 6)\n",
            "Created augmented file: living_room_942_aug.txt (kept 1 of 13)\n",
            "Created augmented file: living_room_861_aug.txt (kept 2 of 3)\n",
            "Created augmented file: living_room_918_aug.txt (kept 3 of 4)\n",
            "Created augmented file: living_room_986_aug.txt (kept 19 of 24)\n",
            "Created augmented file: living_room_874_aug.txt (kept 4 of 5)\n",
            "Created augmented file: living_room_953_aug.txt (kept 9 of 22)\n",
            "Created augmented file: living_room_994_aug.txt (kept 1 of 4)\n",
            "✅ Finished cleaning train_input and creating train_input_aug.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Model traing"
      ],
      "metadata": {
        "id": "4FaiadAPglhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels\"\n",
        "\n",
        "train_input_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels/train_input\"\n",
        "train_input_aug_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels/train_input_aug\"\n",
        "val_input_dir   = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels/val_input\"\n",
        "val_input_aug_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels/val_input_aug\"\n",
        "train_target_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels/train\"\n",
        "val_target_dir   = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels/val\""
      ],
      "metadata": {
        "id": "FPVOtHuAo2dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm  # progress bar\n",
        "\n",
        "def load_labels(path):\n",
        "    \"\"\"Load bounding boxes from a YOLO-format label file.\"\"\"\n",
        "    boxes = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                if len(parts) > 0:\n",
        "                    print(f\"⚠️ Skipped bad line in {path}: {parts}\")\n",
        "                continue\n",
        "            cls, xc, yc, w, h = map(float, parts)\n",
        "            boxes.append([int(cls), xc, yc, w, h])\n",
        "    return torch.tensor(boxes) if boxes else torch.zeros((0,5))\n",
        "\n",
        "def make_all_pairs(input_dir, target_dir, desc=\"Pairing files\"):\n",
        "    \"\"\"\n",
        "    Pairs every file in input_dir with the correct target in target_dir.\n",
        "    Handles '_aug' filenames by stripping '_aug' to find the original target.\n",
        "    Shows a progress bar with tqdm.\n",
        "    \"\"\"\n",
        "    pairs = []\n",
        "    files = [f for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "    for fname in tqdm(files, desc=desc):\n",
        "        input_path = os.path.join(input_dir, fname)\n",
        "\n",
        "        # If the file is an augmented file, strip '_aug' to find the original target\n",
        "        target_fname = fname.replace(\"_aug\", \"\") if \"_aug\" in fname else fname\n",
        "        target_path = os.path.join(target_dir, target_fname)\n",
        "\n",
        "        if os.path.exists(target_path):\n",
        "            X = load_labels(input_path)\n",
        "            Y = load_labels(target_path)\n",
        "            pairs.append((X, Y))\n",
        "        else:\n",
        "            print(f\"⚠️ Skipped {fname}, target {target_fname} not found in {target_dir}\")\n",
        "\n",
        "    return pairs\n",
        "\n",
        "train_pairs = make_all_pairs(train_input_dir, train_target_dir, desc=\"Pairing train_input\")\n",
        "train_pairs += make_all_pairs(train_input_aug_dir, train_target_dir, desc=\"Pairing train_input_aug\")\n",
        "\n",
        "val_pairs = make_all_pairs(val_input_dir, val_target_dir, desc=\"Pairing val_input\")\n",
        "val_pairs += make_all_pairs(val_input_aug_dir, val_target_dir, desc=\"Pairing val_input_aug\")\n",
        "\n",
        "print(f\"Loaded {len(train_pairs)} training pairs, {len(val_pairs)} validation pairs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3-dCeyMpHGW",
        "outputId": "b0e2297f-89c9-443e-9823-e79b63732c50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pairing train_input: 100%|██████████| 2285/2285 [00:11<00:00, 194.08it/s]\n",
            "Pairing train_input_aug: 100%|██████████| 316/316 [00:02<00:00, 153.35it/s]\n",
            "Pairing val_input: 100%|██████████| 404/404 [00:02<00:00, 152.60it/s]\n",
            "Pairing val_input_aug: 100%|██████████| 73/73 [00:00<00:00, 207.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2601 training pairs, 477 validation pairs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Train pairs: {len(train_pairs)}\")\n",
        "print(f\"Validation pairs: {len(val_pairs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gLhqfLhw--2",
        "outputId": "773ff670-578e-42b4-cbe1-96ac79d5686b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pairs: 2601\n",
            "Validation pairs: 477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (X, Y) in enumerate(train_pairs[:10]):\n",
        "    print(f\"Train pair {i}: X={X.shape}, Y={Y.shape}\")\n",
        "for i, (X, Y) in enumerate(val_pairs[:10]):\n",
        "    print(f\"Val pair {i}: X={X.shape}, Y={Y.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAMaFdE3y5pb",
        "outputId": "116a65c6-97b1-4e79-f216-a1282acdeece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train pair 0: X=torch.Size([1, 5]), Y=torch.Size([1, 5])\n",
            "Train pair 1: X=torch.Size([5, 5]), Y=torch.Size([5, 5])\n",
            "Train pair 2: X=torch.Size([18, 5]), Y=torch.Size([19, 5])\n",
            "Train pair 3: X=torch.Size([2, 5]), Y=torch.Size([2, 5])\n",
            "Train pair 4: X=torch.Size([4, 5]), Y=torch.Size([4, 5])\n",
            "Train pair 5: X=torch.Size([7, 5]), Y=torch.Size([7, 5])\n",
            "Train pair 6: X=torch.Size([5, 5]), Y=torch.Size([6, 5])\n",
            "Train pair 7: X=torch.Size([8, 5]), Y=torch.Size([9, 5])\n",
            "Train pair 8: X=torch.Size([9, 5]), Y=torch.Size([11, 5])\n",
            "Train pair 9: X=torch.Size([3, 5]), Y=torch.Size([5, 5])\n",
            "Val pair 0: X=torch.Size([10, 5]), Y=torch.Size([10, 5])\n",
            "Val pair 1: X=torch.Size([9, 5]), Y=torch.Size([10, 5])\n",
            "Val pair 2: X=torch.Size([19, 5]), Y=torch.Size([19, 5])\n",
            "Val pair 3: X=torch.Size([9, 5]), Y=torch.Size([13, 5])\n",
            "Val pair 4: X=torch.Size([8, 5]), Y=torch.Size([10, 5])\n",
            "Val pair 5: X=torch.Size([4, 5]), Y=torch.Size([4, 5])\n",
            "Val pair 6: X=torch.Size([5, 5]), Y=torch.Size([5, 5])\n",
            "Val pair 7: X=torch.Size([10, 5]), Y=torch.Size([14, 5])\n",
            "Val pair 8: X=torch.Size([2, 5]), Y=torch.Size([10, 5])\n",
            "Val pair 9: X=torch.Size([4, 5]), Y=torch.Size([7, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def encode_objects(boxes, num_classes=12):\n",
        "    feats = []\n",
        "    for b in boxes:\n",
        "        cls, xc, yc, w, h = b\n",
        "        if cls == 11:  # skip picture frames\n",
        "            continue\n",
        "        onehot = F.one_hot(torch.tensor(int(cls)), num_classes=num_classes-1)\n",
        "        feats.append(torch.cat([onehot.float(), torch.tensor([xc, yc, w, h])]))\n",
        "    return torch.stack(feats) if feats else torch.zeros((1, num_classes+3))"
      ],
      "metadata": {
        "id": "v0dYcz0KznWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SceneLayoutPredictor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=4, num_pred=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 4 * num_pred)\n",
        "        )\n",
        "        self.num_pred = num_pred\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.encoder(x.unsqueeze(1)).squeeze(1)\n",
        "        scene_feat = x.mean(0)\n",
        "        preds = self.output_head(scene_feat)\n",
        "        return preds.view(self.num_pred, 4)"
      ],
      "metadata": {
        "id": "Tkm_Wn5hz7H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = SceneLayoutPredictor(input_dim=15, num_pred=2).to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def train_epoch(pairs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, Y in pairs:\n",
        "        X_enc = encode_objects(X).to(device)\n",
        "        frame_boxes = Y[Y[:,0]==11][:,1:].to(device)\n",
        "        if len(frame_boxes) == 0:\n",
        "            continue\n",
        "\n",
        "        preds = model(X_enc)\n",
        "        target = torch.zeros_like(preds)\n",
        "        target[:len(frame_boxes)] = frame_boxes[:preds.shape[0]]\n",
        "\n",
        "        loss = F.smooth_l1_loss(preds, target)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(pairs)\n",
        "\n",
        "for epoch in range(20):\n",
        "    loss = train_epoch(train_pairs)\n",
        "    print(f\"Epoch {epoch+1:02d} | Train loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T2VnOvez-fJ",
        "outputId": "2ab1d053-0d24-4a32-8f9f-077c31f01f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train loss: 0.0091\n",
            "Epoch 02 | Train loss: 0.0087\n",
            "Epoch 03 | Train loss: 0.0085\n",
            "Epoch 04 | Train loss: 0.0084\n",
            "Epoch 05 | Train loss: 0.0083\n",
            "Epoch 06 | Train loss: 0.0083\n",
            "Epoch 07 | Train loss: 0.0082\n",
            "Epoch 08 | Train loss: 0.0081\n",
            "Epoch 09 | Train loss: 0.0079\n",
            "Epoch 10 | Train loss: 0.0077\n",
            "Epoch 11 | Train loss: 0.0076\n",
            "Epoch 12 | Train loss: 0.0074\n",
            "Epoch 13 | Train loss: 0.0072\n",
            "Epoch 14 | Train loss: 0.0070\n",
            "Epoch 15 | Train loss: 0.0067\n",
            "Epoch 16 | Train loss: 0.0066\n",
            "Epoch 17 | Train loss: 0.0063\n",
            "Epoch 18 | Train loss: 0.0060\n",
            "Epoch 19 | Train loss: 0.0057\n",
            "Epoch 20 | Train loss: 0.0055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(pairs):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for X, Y in pairs:\n",
        "        X_enc = encode_objects(X).to(device)\n",
        "        frame_boxes = Y[Y[:,0]==11][:,1:].to(device)\n",
        "        if len(frame_boxes) == 0:\n",
        "            continue\n",
        "        preds = model(X_enc)\n",
        "        target = torch.zeros_like(preds)\n",
        "        target[:len(frame_boxes)] = frame_boxes[:preds.shape[0]]\n",
        "        loss = F.smooth_l1_loss(preds, target)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(pairs)\n",
        "\n",
        "val_loss = evaluate(val_pairs)\n",
        "print(f\"Validation loss: {val_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyLp7iRO1Ujd",
        "outputId": "627542b5-b6ac-400f-ef40-77a773231db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 0.0127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def show_boxes(preds, targets, img_w=640, img_h=480):\n",
        "    fig, ax = plt.subplots()\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "\n",
        "    for (x, y, w, h) in preds:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='r', facecolor='none', linewidth=2\n",
        "        ))\n",
        "\n",
        "    for (x, y, w, h) in targets:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='g', facecolor='none', linewidth=2\n",
        "        ))\n",
        "\n",
        "    # Set axes limits explicitly\n",
        "    ax.set_xlim(0, img_w)\n",
        "    ax.set_ylim(img_h, 0)  # invert y-axis for image coordinates\n",
        "    ax.set_aspect('equal')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "X, Y = val_pairs[296]\n",
        "preds = model(encode_objects(X).to(device)).cpu()\n",
        "targets = Y[Y[:,0]==11][:,1:].cpu()\n",
        "show_boxes(preds, targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "pzOBbuIP1VPh",
        "outputId": "47569cc3-322e-402d-bc3a-4294fab15d7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifying original since it appeared to work the best, now weights samples towards ones with class 11 in them"
      ],
      "metadata": {
        "id": "o8aOt7NmjQAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Dataset wrapper for pairs ---\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]\n",
        "\n",
        "train_dataset = PairDataset(train_pairs)\n",
        "\n",
        "# --- Compute sample weights based on presence of class 11 (frames) ---\n",
        "\n",
        "labels = []\n",
        "for _, Y in train_pairs:\n",
        "    has_frame = (Y[:,0] == 11).any().item()\n",
        "    labels.append(1 if has_frame else 0)\n",
        "labels = torch.tensor(labels)\n",
        "class_counts = torch.bincount(labels)\n",
        "weights_per_class = 1. / class_counts.float()\n",
        "sample_weights = weights_per_class[labels]\n",
        "\n",
        "# Create WeightedRandomSampler\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# DataLoader with sampler, batch size=1 as per your original single sample processing\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, sampler=sampler)"
      ],
      "metadata": {
        "id": "SZHgvYNzjU16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Encoding function ---\n",
        "\n",
        "def encode_objects(boxes, num_classes=12):\n",
        "    feats = []\n",
        "    for b in boxes:\n",
        "        cls, xc, yc, w, h = b\n",
        "        if cls == 11:  # skip picture frames\n",
        "            continue\n",
        "        onehot = F.one_hot(torch.tensor(int(cls)), num_classes=num_classes-1)\n",
        "        feats.append(torch.cat([onehot.float(), torch.tensor([xc, yc, w, h])]))\n",
        "    return torch.stack(feats) if feats else torch.zeros((1, num_classes+3))"
      ],
      "metadata": {
        "id": "VcfgXT3ymnF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model definition ---\n",
        "\n",
        "class SceneLayoutPredictor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=4, num_pred=2):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 4 * num_pred)\n",
        "        )\n",
        "        self.num_pred = num_pred\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.encoder(x.unsqueeze(1)).squeeze(1)\n",
        "        scene_feat = x.mean(0)\n",
        "        preds = self.output_head(scene_feat)\n",
        "        return preds.view(self.num_pred, 4)"
      ],
      "metadata": {
        "id": "OEGkKsgfmrtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup device, model, optimizer ---\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SceneLayoutPredictor(input_dim=15, num_pred=2).to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# --- Training loop using DataLoader with weighted sampling ---\n",
        "\n",
        "def train_epoch_loader(loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, Y in loader:\n",
        "        X_enc = encode_objects(X[0]).to(device)\n",
        "        frame_boxes = Y[0][Y[0][:,0]==11][:,1:].to(device)\n",
        "        if len(frame_boxes) == 0:\n",
        "            continue\n",
        "        preds = model(X_enc)\n",
        "        target = torch.zeros_like(preds)\n",
        "        target[:len(frame_boxes)] = frame_boxes[:preds.shape[0]]\n",
        "        loss = F.smooth_l1_loss(preds, target)\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n"
      ],
      "metadata": {
        "id": "rTJjKoYQmwYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Validation function, unchanged ---\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(pairs):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for X, Y in pairs:\n",
        "        X_enc = encode_objects(X).to(device)\n",
        "        frame_boxes = Y[Y[:,0]==11][:,1:].to(device)\n",
        "        if len(frame_boxes) == 0:\n",
        "            continue\n",
        "        preds = model(X_enc)\n",
        "        target = torch.zeros_like(preds)\n",
        "        target[:len(frame_boxes)] = frame_boxes[:preds.shape[0]]\n",
        "        loss = F.smooth_l1_loss(preds, target)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(pairs)"
      ],
      "metadata": {
        "id": "YHeLogRamzkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training execution ---\n",
        "\n",
        "for epoch in range(20):\n",
        "    loss = train_epoch_loader(train_loader)\n",
        "    print(f\"Epoch {epoch+1:02d} | Train loss: {loss:.4f}\")\n",
        "\n",
        "val_loss = evaluate(val_pairs)\n",
        "print(f\"Validation loss: {val_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV6eyrUFm4VT",
        "outputId": "213e132c-bbb3-4f9d-e486-fc812dd2bc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train loss: 0.0091\n",
            "Epoch 02 | Train loss: 0.0084\n",
            "Epoch 03 | Train loss: 0.0090\n",
            "Epoch 04 | Train loss: 0.0080\n",
            "Epoch 05 | Train loss: 0.0079\n",
            "Epoch 06 | Train loss: 0.0077\n",
            "Epoch 07 | Train loss: 0.0076\n",
            "Epoch 08 | Train loss: 0.0077\n",
            "Epoch 09 | Train loss: 0.0075\n",
            "Epoch 10 | Train loss: 0.0074\n",
            "Epoch 11 | Train loss: 0.0071\n",
            "Epoch 12 | Train loss: 0.0071\n",
            "Epoch 13 | Train loss: 0.0067\n",
            "Epoch 14 | Train loss: 0.0062\n",
            "Epoch 15 | Train loss: 0.0063\n",
            "Epoch 16 | Train loss: 0.0064\n",
            "Epoch 17 | Train loss: 0.0065\n",
            "Epoch 18 | Train loss: 0.0059\n",
            "Epoch 19 | Train loss: 0.0056\n",
            "Epoch 20 | Train loss: 0.0057\n",
            "Validation loss: 0.0103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualization ---\n",
        "\n",
        "def show_boxes(preds, targets, img_w=640, img_h=480):\n",
        "    fig, ax = plt.subplots()\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "\n",
        "    for (x, y, w, h) in preds:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='r', facecolor='none', linewidth=2\n",
        "        ))\n",
        "\n",
        "    for (x, y, w, h) in targets:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='g', facecolor='none', linewidth=2\n",
        "        ))\n",
        "\n",
        "    ax.set_xlim(0, img_w)\n",
        "    ax.set_ylim(img_h, 0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.show()\n",
        "\n",
        "# Example prediction visualization\n",
        "X, Y = val_pairs[292]\n",
        "preds = model(encode_objects(X).to(device)).cpu()\n",
        "targets = Y[Y[:,0]==11][:,1:].cpu()\n",
        "show_boxes(preds, targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "VWPfIWh-m7dL",
        "outputId": "4a99f3cc-9eb0-4796-f1b5-aacff4628281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifying to attempt predictions for variable number of paintings. Note large number of paintings usually result in smaller size of them in train/val sets"
      ],
      "metadata": {
        "id": "7nwUZ038nkVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Current method always prediciting 10 paintings per, but filtering them based on cofidence thresholds and then removing overlapping ones"
      ],
      "metadata": {
        "id": "RDRtqJWTqHOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SceneLayoutPredictor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=4, num_pred=10):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 5 * num_pred)  # 4 box coords + 1 confidence\n",
        "        )\n",
        "        self.num_pred = num_pred\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.encoder(x.unsqueeze(1)).squeeze(1)\n",
        "        scene_feat = x.mean(0)\n",
        "        preds = self.output_head(scene_feat)\n",
        "        preds = preds.view(self.num_pred, 5)\n",
        "        boxes = preds[:, :4]\n",
        "        confidences = torch.sigmoid(preds[:, 4])\n",
        "        return boxes, confidences\n"
      ],
      "metadata": {
        "id": "VZS3_MqDqOO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SceneLayoutPredictor(input_dim=15, num_pred=10).to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# --- Loss function ---\n",
        "\n",
        "def compute_loss(pred_boxes, pred_conf, target_boxes):\n",
        "    # Simplified loss: match pred boxes to targets by index (limit to targets or pred count)\n",
        "    n = min(len(target_boxes), pred_boxes.size(0))\n",
        "    if n == 0:\n",
        "        # No target frames, encourage confidence to zero\n",
        "        conf_loss = F.binary_cross_entropy(pred_conf, torch.zeros_like(pred_conf))\n",
        "        box_loss = torch.tensor(0., device=pred_boxes.device)\n",
        "    else:\n",
        "        conf_target = torch.zeros_like(pred_conf)\n",
        "        conf_target[:n] = 1.0\n",
        "        conf_loss = F.binary_cross_entropy(pred_conf, conf_target)\n",
        "        box_loss = F.smooth_l1_loss(pred_boxes[:n], target_boxes[:n])\n",
        "    return conf_loss + box_loss"
      ],
      "metadata": {
        "id": "az1bbiErqvTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training function ---\n",
        "\n",
        "def train_epoch_loader(loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, Y in loader:\n",
        "        X_enc = encode_objects(X[0]).to(device)\n",
        "        frame_boxes = Y[0][Y[0][:,0] == 11][:, 1:].to(device)\n",
        "        opt.zero_grad()\n",
        "        pred_boxes, pred_conf = model(X_enc)\n",
        "        loss = compute_loss(pred_boxes, pred_conf, frame_boxes)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# --- Validation function (no sampling) ---\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(pairs):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for X, Y in pairs:\n",
        "        X_enc = encode_objects(X).to(device)\n",
        "        frame_boxes = Y[Y[:,0] == 11][:, 1:].to(device)\n",
        "        pred_boxes, pred_conf = model(X_enc)\n",
        "        loss = compute_loss(pred_boxes, pred_conf, frame_boxes)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(pairs)"
      ],
      "metadata": {
        "id": "CgEpgk5mq1Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #--- Inference helper with confidence threshold and NMS ---\n",
        "\n",
        "def inference_with_nms(model, X_enc, conf_thresh=0.5, nms_thresh=0.4):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        boxes, confidences = model(X_enc)\n",
        "        keep = confidences > conf_thresh\n",
        "        if keep.sum() == 0:\n",
        "            return torch.empty((0,4), device=X_enc.device)\n",
        "        boxes = boxes[keep]\n",
        "        scores = confidences[keep]\n",
        "        x1 = boxes[:,0] - boxes[:,2]/2\n",
        "        y1 = boxes[:,1] - boxes[:,3]/2\n",
        "        x2 = boxes[:,0] + boxes[:,2]/2\n",
        "        y2 = boxes[:,1] + boxes[:,3]/2\n",
        "        boxes_xyxy = torch.stack([x1, y1, x2, y2], dim=1)\n",
        "        keep_indices = ops.nms(boxes_xyxy, scores, nms_thresh)\n",
        "        return boxes[keep_indices]"
      ],
      "metadata": {
        "id": "zxyjsPnIq5yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training loop ---\n",
        "\n",
        "for epoch in range(20):\n",
        "    train_loss = train_epoch_loader(train_loader)\n",
        "    val_loss = evaluate(val_pairs)\n",
        "    print(f\"Epoch {epoch+1:02d} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZxXayvmrB7u",
        "outputId": "77f95e74-e374-4985-8a0a-d56aa9e63c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train loss: 0.3646 | Val loss: 0.3934\n",
            "Epoch 02 | Train loss: 0.3499 | Val loss: 0.4179\n",
            "Epoch 03 | Train loss: 0.3480 | Val loss: 0.3869\n",
            "Epoch 04 | Train loss: 0.3558 | Val loss: 0.3871\n",
            "Epoch 05 | Train loss: 0.3582 | Val loss: 0.4000\n",
            "Epoch 06 | Train loss: 0.3226 | Val loss: 0.3986\n",
            "Epoch 07 | Train loss: 0.3345 | Val loss: 0.3951\n",
            "Epoch 08 | Train loss: 0.3555 | Val loss: 0.3964\n",
            "Epoch 09 | Train loss: 0.3402 | Val loss: 0.3881\n",
            "Epoch 10 | Train loss: 0.3413 | Val loss: 0.3897\n",
            "Epoch 11 | Train loss: 0.3525 | Val loss: 0.3984\n",
            "Epoch 12 | Train loss: 0.3448 | Val loss: 0.3853\n",
            "Epoch 13 | Train loss: 0.3449 | Val loss: 0.3893\n",
            "Epoch 14 | Train loss: 0.3347 | Val loss: 0.3900\n",
            "Epoch 15 | Train loss: 0.3350 | Val loss: 0.3867\n",
            "Epoch 16 | Train loss: 0.3403 | Val loss: 0.3894\n",
            "Epoch 17 | Train loss: 0.3441 | Val loss: 0.4256\n",
            "Epoch 18 | Train loss: 0.3393 | Val loss: 0.3885\n",
            "Epoch 19 | Train loss: 0.3541 | Val loss: 0.3932\n",
            "Epoch 20 | Train loss: 0.3410 | Val loss: 0.3957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualization (unchanged) ---\n",
        "import torchvision.ops as ops\n",
        "\n",
        "\n",
        "def show_boxes(preds, targets, img_w=640, img_h=480):\n",
        "    fig, ax = plt.subplots()\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    for (x, y, w, h) in preds:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='r', facecolor='none', linewidth=2))\n",
        "    for (x, y, w, h) in targets:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='g', facecolor='none', linewidth=2))\n",
        "    ax.set_xlim(0, img_w)\n",
        "    ax.set_ylim(img_h, 0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.show()\n",
        "\n",
        "X, Y = val_pairs[292]\n",
        "X_enc = encode_objects(X).to(device)\n",
        "predicted_boxes = inference_with_nms(model, X_enc, conf_thresh=0.5, nms_thresh=0.4)\n",
        "target_boxes = Y[Y[:,0] == 11][:, 1:].to(device)\n",
        "show_boxes(predicted_boxes, target_boxes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "Ooa49ZS5rCNN",
        "outputId": "b67ba689-103f-48b0-ba6d-4639ceb4139b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Updating to IoU, ran with 20 num_pred"
      ],
      "metadata": {
        "id": "TYNclGYrw9Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6p6iJ_uKxPLw",
        "outputId": "1858f17f-e875-4c84-c83b-4217ba53a2b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (25.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from torchmetrics) (2.8.0+cu126)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.3)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.15.2 torchmetrics-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.ops as ops\n",
        "from torchmetrics.detection import IntersectionOverUnion\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ],
      "metadata": {
        "id": "DTHmyJhkxAXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Dataset wrapper ---\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]\n",
        "\n",
        "train_dataset = PairDataset(train_pairs)\n",
        "\n",
        "# --- Sample weights for imbalance handling ---\n",
        "\n",
        "labels = []\n",
        "for _, Y in train_pairs:\n",
        "    has_frame = (Y[:,0] == 11).any().item()\n",
        "    labels.append(1 if has_frame else 0)\n",
        "\n",
        "labels = torch.tensor(labels)\n",
        "class_counts = torch.bincount(labels)\n",
        "weights_per_class = 1. / class_counts.float()\n",
        "sample_weights = weights_per_class[labels]\n",
        "\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, sampler=sampler)\n",
        "\n",
        "# --- Encoding objects ---\n",
        "\n",
        "def encode_objects(boxes, num_classes=12):\n",
        "    feats = []\n",
        "    for b in boxes:\n",
        "        cls, xc, yc, w, h = b\n",
        "        if cls == 11:\n",
        "            continue\n",
        "        onehot = F.one_hot(torch.tensor(int(cls)), num_classes=num_classes-1)\n",
        "        feats.append(torch.cat([onehot.float(), torch.tensor([xc, yc, w, h])]))\n",
        "    return torch.stack(feats) if feats else torch.zeros((1, num_classes+3))"
      ],
      "metadata": {
        "id": "m-yoh6NQxEDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model with boxes + confidence ---\n",
        "\n",
        "class SceneLayoutPredictor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=4, num_pred=20):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 5 * num_pred)  # 4 coords + 1 conf\n",
        "        )\n",
        "        self.num_pred = num_pred\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.encoder(x.unsqueeze(1)).squeeze(1)\n",
        "        scene_feat = x.mean(0)\n",
        "        preds = self.output_head(scene_feat).view(self.num_pred, 5)\n",
        "        boxes = preds[:, :4]\n",
        "        confidences = torch.sigmoid(preds[:, 4])\n",
        "        return boxes, confidences\n"
      ],
      "metadata": {
        "id": "P4jYplRrxjZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Setup model, optimizer, device ---\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SceneLayoutPredictor(input_dim=15, num_pred=20).to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# --- Loss function combining box regression & confidence ---\n",
        "\n",
        "def compute_loss(pred_boxes, pred_conf, target_boxes):\n",
        "    n = min(len(target_boxes), pred_boxes.size(0))\n",
        "    if n == 0:\n",
        "        conf_loss = F.binary_cross_entropy(pred_conf, torch.zeros_like(pred_conf))\n",
        "        box_loss = torch.tensor(0., device=pred_boxes.device)\n",
        "    else:\n",
        "        conf_target = torch.zeros_like(pred_conf)\n",
        "        conf_target[:n] = 1.0\n",
        "        conf_loss = F.binary_cross_entropy(pred_conf, conf_target)\n",
        "        box_loss = F.smooth_l1_loss(pred_boxes[:n], target_boxes[:n])\n",
        "    return conf_loss + box_loss"
      ],
      "metadata": {
        "id": "zUGG_9b1xjvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- IoU metric ---\n",
        "\n",
        "iou_metric = IntersectionOverUnion(box_format='xyxy')\n",
        "\n",
        "def boxes_cxcywh_to_xyxy(boxes):\n",
        "    cx, cy, w, h = boxes.unbind(-1)\n",
        "    return torch.stack([cx - w/2, cy - h/2, cx + w/2, cy + h/2], dim=-1)"
      ],
      "metadata": {
        "id": "Gvr-RD_DxroO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training epoch ---\n",
        "\n",
        "def train_epoch_loader(loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, Y in loader:\n",
        "        X_enc = encode_objects(X[0]).to(device)\n",
        "        frame_boxes = Y[0][Y[0][:,0] == 11][:,1:].to(device)\n",
        "        opt.zero_grad()\n",
        "        pred_boxes, pred_conf = model(X_enc)\n",
        "        loss = compute_loss(pred_boxes, pred_conf, frame_boxes)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# --- Validation with IoU ---\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_iou(pairs):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    iou_metric.reset()\n",
        "    for X, Y in pairs:\n",
        "        X_enc = encode_objects(X).to(device)\n",
        "        target_boxes = Y[Y[:,0] == 11][:,1:].to(device)\n",
        "\n",
        "        pred_boxes, pred_conf = model(X_enc)\n",
        "        conf_thresh = 0.5\n",
        "        nms_thresh = 0.4\n",
        "        keep = pred_conf > conf_thresh\n",
        "        if keep.sum() == 0:\n",
        "            continue\n",
        "        pred_boxes = pred_boxes[keep]\n",
        "\n",
        "        # NMS requires xyxy boxes\n",
        "        boxes_xyxy = boxes_cxcywh_to_xyxy(pred_boxes)\n",
        "        scores = pred_conf[keep]\n",
        "        keep_indices = ops.nms(boxes_xyxy, scores, nms_thresh)\n",
        "        pred_boxes = pred_boxes[keep_indices]\n",
        "\n",
        "        target_boxes_xyxy = boxes_cxcywh_to_xyxy(target_boxes)\n",
        "        pred_boxes_xyxy = boxes_cxcywh_to_xyxy(pred_boxes)\n",
        "\n",
        "        preds = [{'boxes': pred_boxes_xyxy, 'labels': torch.zeros(len(pred_boxes_xyxy), dtype=torch.int, device=device)}]\n",
        "        targets = [{'boxes': target_boxes_xyxy, 'labels': torch.zeros(len(target_boxes_xyxy), dtype=torch.int, device=device)}]\n",
        "\n",
        "        iou_metric.update(preds, targets)\n",
        "\n",
        "        loss = compute_loss(pred_boxes, pred_conf[keep][keep_indices], target_boxes)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    mean_iou = iou_metric.compute()['iou'].item()\n",
        "    avg_loss = total_loss / len(pairs)\n",
        "    return avg_loss, mean_iou"
      ],
      "metadata": {
        "id": "bSNYSUryx0n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training loop ---\n",
        "\n",
        "for epoch in range(20):\n",
        "    train_loss = train_epoch_loader(train_loader)\n",
        "    val_loss, val_iou = evaluate_with_iou(val_pairs)\n",
        "    print(f\"Epoch {epoch+1:02d} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | Val mean IoU: {val_iou:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz6kU-tjx542",
        "outputId": "f8f7a6f1-431e-4225-f994-745aa767f277"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train loss: 0.2246 | Val loss: 0.5363 | Val mean IoU: 0.0213\n",
            "Epoch 02 | Train loss: 0.2023 | Val loss: 0.6759 | Val mean IoU: 0.0270\n",
            "Epoch 03 | Train loss: 0.2008 | Val loss: 0.6483 | Val mean IoU: 0.0226\n",
            "Epoch 04 | Train loss: 0.2070 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 05 | Train loss: 0.2159 | Val loss: 0.2841 | Val mean IoU: 0.0178\n",
            "Epoch 06 | Train loss: 0.1922 | Val loss: 0.2347 | Val mean IoU: 0.0392\n",
            "Epoch 07 | Train loss: 0.1916 | Val loss: 0.6532 | Val mean IoU: 0.0241\n",
            "Epoch 08 | Train loss: 0.2075 | Val loss: 0.6398 | Val mean IoU: 0.0262\n",
            "Epoch 09 | Train loss: 0.2042 | Val loss: 0.3222 | Val mean IoU: 0.0303\n",
            "Epoch 10 | Train loss: 0.2143 | Val loss: 0.6853 | Val mean IoU: 0.0286\n",
            "Epoch 11 | Train loss: 0.2073 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 12 | Train loss: 0.2088 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 13 | Train loss: 0.2027 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 14 | Train loss: 0.2142 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 15 | Train loss: 0.2014 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 16 | Train loss: 0.2058 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 17 | Train loss: 0.2019 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 18 | Train loss: 0.2068 | Val loss: 0.0033 | Val mean IoU: 0.0388\n",
            "Epoch 19 | Train loss: 0.2154 | Val loss: 0.7007 | Val mean IoU: 0.0256\n",
            "Epoch 20 | Train loss: 0.2080 | Val loss: 0.6967 | Val mean IoU: 0.0354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualization ---\n",
        "\n",
        "def show_boxes(preds, targets, img_w=640, img_h=480):\n",
        "    fig, ax = plt.subplots()\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    for (x, y, w, h) in preds:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='r', facecolor='none', linewidth=2))\n",
        "    for (x, y, w, h) in targets:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='g', facecolor='none', linewidth=2))\n",
        "    ax.set_xlim(0, img_w)\n",
        "    ax.set_ylim(img_h, 0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.show()\n",
        "\n",
        "    # --- Example prediction visualization---\n",
        "\n",
        "X, Y = val_pairs[292]\n",
        "X_enc = encode_objects(X).to(device)\n",
        "pred_boxes, pred_conf = model(X_enc)\n",
        "\n",
        "# Apply confidence threshold and NMS for visualization\n",
        "conf_thresh = 0.5\n",
        "nms_thresh = 0.4\n",
        "keep = pred_conf > conf_thresh\n",
        "pred_boxes = pred_boxes[keep]\n",
        "scores = pred_conf[keep]\n",
        "\n",
        "boxes_xyxy = boxes_cxcywh_to_xyxy(pred_boxes)\n",
        "keep_indices = ops.nms(boxes_xyxy, scores, nms_thresh)\n",
        "pred_boxes = pred_boxes[keep_indices]\n",
        "\n",
        "target_boxes = Y[Y[:,0]==11][:,1:].to(device)\n",
        "\n",
        "show_boxes(pred_boxes, target_boxes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "Q2UJKCd0yDK-",
        "outputId": "fd782a4d-a665-4326-9bc0-98dd3168596f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using GIoU #Currently not working"
      ],
      "metadata": {
        "id": "tTLafYM634R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.ops import generalized_box_iou_loss\n",
        "# --- Dataset wrapper ---\n",
        "\n",
        "class PairDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]\n",
        "\n",
        "train_dataset = PairDataset(train_pairs)\n",
        "\n",
        "# --- Weighted sampling ---\n",
        "\n",
        "labels = []\n",
        "for _, Y in train_pairs:\n",
        "    has_frame = (Y[:,0] == 11).any().item()\n",
        "    labels.append(1 if has_frame else 0)\n",
        "\n",
        "labels = torch.tensor(labels)\n",
        "class_counts = torch.bincount(labels)\n",
        "weights_per_class = 1. / class_counts.float()\n",
        "sample_weights = weights_per_class[labels]\n",
        "\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, sampler=sampler)\n",
        "\n",
        "# --- Encoding ---\n",
        "\n",
        "def encode_objects(boxes, num_classes=12):\n",
        "    feats = []\n",
        "    for b in boxes:\n",
        "        cls, xc, yc, w, h = b\n",
        "        if cls == 11:\n",
        "            continue\n",
        "        onehot = F.one_hot(torch.tensor(int(cls)), num_classes=num_classes-1)\n",
        "        feats.append(torch.cat([onehot.float(), torch.tensor([xc, yc, w, h])]))\n",
        "    return torch.stack(feats) if feats else torch.zeros((1, num_classes+3))\n",
        "\n",
        "# --- Model ---\n",
        "\n",
        "class SceneLayoutPredictor(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=128, num_layers=4, num_pred=20):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=8),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
        "        self.output_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 5 * num_pred)\n",
        "        )\n",
        "        self.num_pred = num_pred\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.encoder(x.unsqueeze(1)).squeeze(1)\n",
        "        scene_feat = x.mean(0)\n",
        "        preds = self.output_head(scene_feat).view(self.num_pred, 5)\n",
        "        boxes = preds[:, :4]\n",
        "        confidences = torch.sigmoid(preds[:, 4])\n",
        "        return boxes, confidences\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SceneLayoutPredictor(input_dim=15, num_pred=20).to(device)\n",
        "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# --- Box format conversion and clamping ---\n",
        "\n",
        "def cxcywh_to_xyxy(clamped_boxes):\n",
        "    cx, cy, w, h = clamped_boxes.unbind(-1)\n",
        "    xmin = cx - 0.5 * w\n",
        "    ymin = cy - 0.5 * h\n",
        "    xmax = cx + 0.5 * w\n",
        "    ymax = cy + 0.5 * h\n",
        "    boxes_xyxy = torch.stack((xmin, ymin, xmax, ymax), dim=-1)\n",
        "    # Clamp to [0,1]\n",
        "    boxes_xyxy = boxes_xyxy.clamp(0.0, 1.0)\n",
        "    return boxes_xyxy\n",
        "\n",
        "def clamp_boxes(boxes, min_val=0.0, max_val=1.0, eps=1e-6):\n",
        "    boxes_clamped = boxes.clone()\n",
        "    boxes_clamped[:, 0] = boxes[:, 0].clamp(min_val, max_val)  # cx\n",
        "    boxes_clamped[:, 1] = boxes[:, 1].clamp(min_val, max_val)  # cy\n",
        "    boxes_clamped[:, 2] = boxes[:, 2].clamp(min=eps)           # w\n",
        "    boxes_clamped[:, 3] = boxes[:, 3].clamp(min=eps)           # h\n",
        "    return boxes_clamped\n",
        "\n",
        "# --- Loss calculation using GIoU and clamping/conversion ---\n",
        "\n",
        "def compute_loss(pred_boxes, pred_conf, target_boxes):\n",
        "    n = min(len(target_boxes), pred_boxes.size(0))\n",
        "    if n == 0:\n",
        "        conf_loss = F.binary_cross_entropy(pred_conf, torch.zeros_like(pred_conf))\n",
        "        box_loss = torch.tensor(0., device=pred_boxes.device)\n",
        "    else:\n",
        "        conf_target = torch.zeros_like(pred_conf)\n",
        "        conf_target[:n] = 1.0\n",
        "        conf_loss = F.binary_cross_entropy(pred_conf, conf_target)\n",
        "\n",
        "        pred_boxes_clamped = clamp_boxes(pred_boxes[:n])\n",
        "        target_boxes_clamped = clamp_boxes(target_boxes[:n])\n",
        "\n",
        "        pred_boxes_xyxy = cxcywh_to_xyxy(pred_boxes_clamped)\n",
        "        target_boxes_xyxy = cxcywh_to_xyxy(target_boxes_clamped)\n",
        "\n",
        "        box_loss = generalized_box_iou_loss(pred_boxes_xyxy, target_boxes_xyxy, reduction=\"mean\")\n",
        "\n",
        "    total_loss = conf_loss + box_loss\n",
        "    assert total_loss.dim() == 0, f\"Loss is not scalar: {total_loss.shape}\"\n",
        "    return total_loss\n",
        "\n",
        "# --- Training loop ---\n",
        "\n",
        "def train_epoch_loader(loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for X, Y in loader:\n",
        "        X_enc = encode_objects(X[0]).to(device)\n",
        "        frame_boxes = Y[0][Y[0][:,0] == 11][:, 1:].to(device)\n",
        "        opt.zero_grad()\n",
        "        pred_boxes, pred_conf = model(X_enc)\n",
        "        loss = compute_loss(pred_boxes, pred_conf, frame_boxes)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# --- Evaluation with IoU metric ---\n",
        "\n",
        "iou_metric = IntersectionOverUnion(box_format=\"xyxy\")\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_iou(pairs):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    iou_metric.reset()\n",
        "    for X, Y in pairs:\n",
        "        X_enc = encode_objects(X).to(device)\n",
        "        target_boxes = Y[Y[:,0] == 11][:, 1:].to(device)\n",
        "        pred_boxes, pred_conf = model(X_enc)\n",
        "\n",
        "        conf_thresh = 0.5\n",
        "        nms_thresh = 0.4\n",
        "        keep = pred_conf > conf_thresh\n",
        "        if keep.sum() == 0:\n",
        "            continue\n",
        "        pred_boxes_filtered = pred_boxes[keep]\n",
        "        scores = pred_conf[keep]\n",
        "\n",
        "        pred_boxes_clamped = clamp_boxes(pred_boxes_filtered)\n",
        "        pred_boxes_xyxy = cxcywh_to_xyxy(pred_boxes_clamped)\n",
        "        keep_indices = ops.nms(pred_boxes_xyxy, scores, nms_thresh)\n",
        "        pred_boxes_nms = pred_boxes_filtered[keep_indices]\n",
        "\n",
        "        pred_boxes_nms_clamped = clamp_boxes(pred_boxes_nms)\n",
        "        pred_boxes_nms_xyxy = cxcywh_to_xyxy(pred_boxes_nms_clamped)\n",
        "        target_boxes_clamped = clamp_boxes(target_boxes)\n",
        "        target_boxes_xyxy = cxcywh_to_xyxy(target_boxes_clamped)\n",
        "\n",
        "        preds = [{'boxes': pred_boxes_nms_xyxy, 'labels': torch.zeros(len(pred_boxes_nms_xyxy), dtype=torch.int, device=device)}]\n",
        "        targets = [{'boxes': target_boxes_xyxy, 'labels': torch.zeros(len(target_boxes_xyxy), dtype=torch.int, device=device)}]\n",
        "\n",
        "        iou_metric.update(preds, targets)\n",
        "\n",
        "        loss = compute_loss(pred_boxes_nms, pred_conf[keep][keep_indices], target_boxes)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    mean_iou = iou_metric.compute()['iou'].item()\n",
        "    avg_loss = total_loss / len(pairs)\n",
        "    return avg_loss, mean_iou\n",
        "\n",
        "# --- Visualization omitted, same as before ---\n",
        "\n",
        "# --- Run training ---\n",
        "\n",
        "for epoch in range(20):\n",
        "    train_loss = train_epoch_loader(train_loader)\n",
        "    val_loss, val_iou = evaluate_with_iou(val_pairs)\n",
        "    print(f\"Epoch {epoch+1:02d} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | Val mean IoU: {val_iou:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v-RBuMs53on",
        "outputId": "426e0d53-2407-47ea-ffc9-ec09fb5b1557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | Train loss: 0.8328 | Val loss: 0.9594 | Val mean IoU: 0.0147\n",
            "Epoch 02 | Train loss: 0.7432 | Val loss: 1.2287 | Val mean IoU: 0.0154\n",
            "Epoch 03 | Train loss: 0.7532 | Val loss: 0.7582 | Val mean IoU: 0.0144\n",
            "Epoch 04 | Train loss: 0.7664 | Val loss: 1.1227 | Val mean IoU: 0.0151\n",
            "Epoch 05 | Train loss: 0.7894 | Val loss: 1.0979 | Val mean IoU: 0.0152\n",
            "Epoch 06 | Train loss: 0.7492 | Val loss: 0.1003 | Val mean IoU: 0.0108\n",
            "Epoch 07 | Train loss: 0.7703 | Val loss: 1.2707 | Val mean IoU: 0.0157\n",
            "Epoch 08 | Train loss: 0.7314 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 09 | Train loss: 0.7528 | Val loss: 1.2601 | Val mean IoU: 0.0157\n",
            "Epoch 10 | Train loss: 0.7521 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 11 | Train loss: 0.7657 | Val loss: 1.2058 | Val mean IoU: 0.0155\n",
            "Epoch 12 | Train loss: 0.7472 | Val loss: 1.2692 | Val mean IoU: 0.0157\n",
            "Epoch 13 | Train loss: 0.7665 | Val loss: 0.4535 | Val mean IoU: 0.0112\n",
            "Epoch 14 | Train loss: 0.7722 | Val loss: 1.2520 | Val mean IoU: 0.0157\n",
            "Epoch 15 | Train loss: 0.7553 | Val loss: 0.0023 | Val mean IoU: 0.2527\n",
            "Epoch 16 | Train loss: 0.7564 | Val loss: 1.2533 | Val mean IoU: 0.0157\n",
            "Epoch 17 | Train loss: 0.7761 | Val loss: 0.0000 | Val mean IoU: 0.0000\n",
            "Epoch 18 | Train loss: 0.7229 | Val loss: 1.2669 | Val mean IoU: 0.0157\n",
            "Epoch 19 | Train loss: 0.7450 | Val loss: 1.2672 | Val mean IoU: 0.0157\n",
            "Epoch 20 | Train loss: 0.7196 | Val loss: 0.0000 | Val mean IoU: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualization ---\n",
        "\n",
        "def show_boxes(preds, targets, img_w=640, img_h=480):\n",
        "    fig, ax = plt.subplots()\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    targets = targets.detach().cpu().numpy()\n",
        "    for (x, y, w, h) in preds:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='r', facecolor='none', linewidth=2))\n",
        "    for (x, y, w, h) in targets:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='g', facecolor='none', linewidth=2))\n",
        "    ax.set_xlim(0, img_w)\n",
        "    ax.set_ylim(img_h, 0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.show()\n",
        "\n",
        "# --- Example prediction visualization ---\n",
        "\n",
        "X, Y = val_pairs[291]\n",
        "X_enc = encode_objects(X).to(device)\n",
        "pred_boxes, pred_conf = model(X_enc)\n",
        "conf_thresh = 0.1\n",
        "nms_thresh = 0.4\n",
        "keep = pred_conf > conf_thresh\n",
        "pred_boxes = pred_boxes[keep]\n",
        "scores = pred_conf[keep]\n",
        "\n",
        "pred_boxes_xyxy = boxes_cxcywh_to_xyxy(pred_boxes)\n",
        "keep_indices = ops.nms(pred_boxes_xyxy, scores, nms_thresh)\n",
        "pred_boxes = pred_boxes[keep_indices]\n",
        "\n",
        "target_boxes = Y[Y[:,0] == 11][:, 1:].to(device)\n",
        "show_boxes(pred_boxes, target_boxes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "sPVmfd3q4Kl1",
        "outputId": "5e667ba1-8bee-40bc-ef40-7679228dc52f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiQAAAGiCAYAAADX8t0oAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAH6lJREFUeJzt3XtwlNX9x/FPQshy3Y0BsklIgnSkQspFDRq22v5aSYkYrdbYQYfaaBkdaKAiDtW0CNVewmDHCxahtS3QUUpLp6BQATNBQy0hQIQaQCNW2oTLJiiTbKCSQHJ+fzCsXQFlAfNl8f2aeQbynLPZ85yJ7nuW3U2cc84JAADAULz1AgAAAAgSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGDONEjmzZunSy+9VN26dVNubq42bdpkuRwAAGDELEj+9Kc/adq0aZo1a5beeOMNjRgxQvn5+WpsbLRaEgAAMBJn9cv1cnNzdfXVV+tXv/qVJKmjo0OZmZmaMmWKHn74YYslAQAAIwkWd9rW1qbq6mqVlJSEz8XHxysvL0+VlZUnzW9tbVVra2v4646ODh08eFB9+vRRXFxcp6wZAABEzzmnlpYWpaenKz7+9P8wYxIk77//vtrb2+X3+yPO+/1+vf322yfNLy0t1aOPPtpZywMAAOdZfX29MjIyTjtuEiTRKikp0bRp08JfNzc3KysrS/X19fJ6vYYrw+fS4MHS/v1SWpp0ioAGAHwkFAopMzNTvXv3/sR5JkHSt29fdenSRQ0NDRHnGxoalJqaetJ8j8cjj8dz0nmv10uQoPOdeMoxPl7i5w8AzsinvcTC5F02iYmJysnJUXl5efhcR0eHysvLFQgELJYEAAAMmf2TzbRp01RUVKSRI0fqmmuu0VNPPaXDhw/rnnvusVoSAAAwYhYk48aN04EDBzRz5kwFg0FdccUVWrNmzUkvdAUAABc/s88hORehUEg+n0/Nzc28hgSdLyND2rtX6t9f2rPHejUAcEE708dsfpcNAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADCXYL0AxJAnnjh+fN7t3//RnxkZtmsBgAtdR8cZTSNIcGaeeEJ68EHrVVxYOjqkvXutVwEAFwWCBGfmf58Z6d/fbh0Xgv37j8dIfLyUlma9GgC4sHV0fPTM8icgSBCd/v2lPXusV2ErI+P4MyNpaewFAHyaUEjy+T51Gi9qBQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOaiDpL169fr5ptvVnp6uuLi4rRixYqIceecZs6cqbS0NHXv3l15eXnatWtXxJyDBw9q/Pjx8nq9SkpK0oQJE3To0KFzuhAAABC7og6Sw4cPa8SIEZo3b94px+fMmaO5c+dqwYIFqqqqUs+ePZWfn68jR46E54wfP147duxQWVmZVq1apfXr1+u+++47+6sAAAAxLSHaG4wdO1Zjx4495ZhzTk899ZRmzJihW265RZL0hz/8QX6/XytWrNAdd9yht956S2vWrNHmzZs1cuRISdIzzzyjG2+8Ub/85S+Vnp5+DpcDAABi0Xl9Dcnu3bsVDAaVl5cXPufz+ZSbm6vKykpJUmVlpZKSksIxIkl5eXmKj49XVVXVKb9va2urQqFQxAEAAC4e5zVIgsGgJMnv90ec9/v94bFgMKiUlJSI8YSEBCUnJ4fnfFxpaal8Pl/4yMzMPJ/LBgAAxmLiXTYlJSVqbm4OH/X19dZLAgAA59F5DZLU1FRJUkNDQ8T5hoaG8FhqaqoaGxsjxo8dO6aDBw+G53ycx+OR1+uNOAAAwMXjvAbJwIEDlZqaqvLy8vC5UCikqqoqBQIBSVIgEFBTU5Oqq6vDc9atW6eOjg7l5uaez+UAAIAYEfW7bA4dOqR33303/PXu3bu1bds2JScnKysrS1OnTtXPfvYzDRo0SAMHDtQjjzyi9PR03XrrrZKkIUOG6IYbbtC9996rBQsW6OjRo5o8ebLuuOMO3mEDAMDnVNRBsmXLFn39618Pfz1t2jRJUlFRkRYtWqQf/vCHOnz4sO677z41NTXpuuuu05o1a9StW7fwbV544QVNnjxZo0ePVnx8vAoLCzV37tzzcDkAACAWxTnnnPUiohUKheTz+dTc3MzrSTpLRoa0d6/Uv7+0Z4/1amyxFwBwxs70MTsm3mUDAAAubgQJAAAwR5AAAABzBAkAADAX9btsLiiDB0vxNFWn2L//oz8zMmzXYu3EXgAAzpvYDhIeGDpfR8fxd5gAAHAexXaQpKXxDEln2b//eIzExx/fd0in+VUHAIDoxXaQvP22xOeQdI4Tn72RlsZnbwAAzjueXgAAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgjiABAADmCBIAAGCOIAEAAOYIEgAAYI4gAQAA5ggSAABgLqogKS0t1dVXX63evXsrJSVFt956q2prayPmHDlyRMXFxerTp4969eqlwsJCNTQ0RMypq6tTQUGBevTooZSUFE2fPl3Hjh0796sBAAAxKaogqaioUHFxsTZu3KiysjIdPXpUY8aM0eHDh8NzHnjgAa1cuVLLli1TRUWF9u3bp9tuuy083t7eroKCArW1tWnDhg1avHixFi1apJkzZ56/qwIAADElzjnnzvbGBw4cUEpKiioqKvTVr35Vzc3N6tevn5YsWaLbb79dkvT2229ryJAhqqys1KhRo7R69WrddNNN2rdvn/x+vyRpwYIFeuihh3TgwAElJiZ+6v2GQiH5fD41NzfL6/We7fIRjYwMae9eqX9/ac8e69UAAGLEmT5mn9NrSJqbmyVJycnJkqTq6modPXpUeXl54TmDBw9WVlaWKisrJUmVlZUaNmxYOEYkKT8/X6FQSDt27Djl/bS2tioUCkUcAADg4nHWQdLR0aGpU6fq2muv1dChQyVJwWBQiYmJSkpKipjr9/sVDAbDc/43Rk6Mnxg7ldLSUvl8vvCRmZl5tssGAAAXoLMOkuLiYm3fvl1Lly49n+s5pZKSEjU3N4eP+vr6z/w+AQBA50k4mxtNnjxZq1at0vr165WRkRE+n5qaqra2NjU1NUU8S9LQ0KDU1NTwnE2bNkV8vxPvwjkx5+M8Ho88Hs/ZLBUAAMSAqJ4hcc5p8uTJWr58udatW6eBAwdGjOfk5Khr164qLy8Pn6utrVVdXZ0CgYAkKRAIqKamRo2NjeE5ZWVl8nq9ys7OPpdrAQAAMSqqZ0iKi4u1ZMkSvfjii+rdu3f4NR8+n0/du3eXz+fThAkTNG3aNCUnJ8vr9WrKlCkKBAIaNWqUJGnMmDHKzs7WXXfdpTlz5igYDGrGjBkqLi7mWRAAAD6nogqS+fPnS5K+9rWvRZxfuHCh7r77bknSk08+qfj4eBUWFqq1tVX5+fl69tlnw3O7dOmiVatWadKkSQoEAurZs6eKior02GOPnduVAACAmHVOn0Nihc8hMcDnkAAAzkKnfA4JAADA+UCQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHNRBcn8+fM1fPhweb1eeb1eBQIBrV69Ojx+5MgRFRcXq0+fPurVq5cKCwvV0NAQ8T3q6upUUFCgHj16KCUlRdOnT9exY8fOz9UAAICYFFWQZGRkaPbs2aqurtaWLVt0/fXX65ZbbtGOHTskSQ888IBWrlypZcuWqaKiQvv27dNtt90Wvn17e7sKCgrU1tamDRs2aPHixVq0aJFmzpx5fq8KAADElDjnnDuXb5CcnKzHH39ct99+u/r166clS5bo9ttvlyS9/fbbGjJkiCorKzVq1CitXr1aN910k/bt2ye/3y9JWrBggR566CEdOHBAiYmJZ3SfoVBIPp9Pzc3N8nq957J8nKmMDGnvXql/f2nPHuvVAABixJk+Zp/1a0ja29u1dOlSHT58WIFAQNXV1Tp69Kjy8vLCcwYPHqysrCxVVlZKkiorKzVs2LBwjEhSfn6+QqFQ+FmWU2ltbVUoFIo4AADAxSPqIKmpqVGvXr3k8Xg0ceJELV++XNnZ2QoGg0pMTFRSUlLEfL/fr2AwKEkKBoMRMXJi/MTY6ZSWlsrn84WPzMzMaJcNAAAuYFEHyeWXX65t27apqqpKkyZNUlFRkXbu3PlZrC2spKREzc3N4aO+vv4zvT8AANC5EqK9QWJioi677DJJUk5OjjZv3qynn35a48aNU1tbm5qamiKeJWloaFBqaqokKTU1VZs2bYr4fifehXNizql4PB55PJ5olwoAAGLEOX8OSUdHh1pbW5WTk6OuXbuqvLw8PFZbW6u6ujoFAgFJUiAQUE1NjRobG8NzysrK5PV6lZ2dfa5LAQAAMSqqZ0hKSko0duxYZWVlqaWlRUuWLNFrr72mtWvXyufzacKECZo2bZqSk5Pl9Xo1ZcoUBQIBjRo1SpI0ZswYZWdn66677tKcOXMUDAY1Y8YMFRcX8wwIAACfY1EFSWNjo7773e9q//798vl8Gj58uNauXatvfOMbkqQnn3xS8fHxKiwsVGtrq/Lz8/Xss8+Gb9+lSxetWrVKkyZNUiAQUM+ePVVUVKTHHnvs/F4VAACIKef8OSQW+BwSA3wOCQDgLHzmn0MCAABwvhAkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwBxBAgAAzBEkAADAHEECAADMESQAAMAcQQIAAMwRJAAAwNw5Bcns2bMVFxenqVOnhs8dOXJExcXF6tOnj3r16qXCwkI1NDRE3K6urk4FBQXq0aOHUlJSNH36dB07duxclgIAAGLYWQfJ5s2b9etf/1rDhw+POP/AAw9o5cqVWrZsmSoqKrRv3z7ddttt4fH29nYVFBSora1NGzZs0OLFi7Vo0SLNnDnz7K8CAADEtLMKkkOHDmn8+PF67rnndMkll4TPNzc363e/+52eeOIJXX/99crJydHChQu1YcMGbdy4UZL0yiuvaOfOnXr++ed1xRVXaOzYsfrpT3+qefPmqa2t7fxcFQAAiClnFSTFxcUqKChQXl5exPnq6modPXo04vzgwYOVlZWlyspKSVJlZaWGDRsmv98fnpOfn69QKKQdO3ac8v5aW1sVCoUiDgAAcPFIiPYGS5cu1RtvvKHNmzefNBYMBpWYmKikpKSI836/X8FgMDznf2PkxPiJsVMpLS3Vo48+Gu1SAQBAjIjqGZL6+nrdf//9euGFF9StW7fPak0nKSkpUXNzc/ior6/vtPsGAACfvaiCpLq6Wo2NjbrqqquUkJCghIQEVVRUaO7cuUpISJDf71dbW5uampoibtfQ0KDU1FRJUmpq6knvujnx9Yk5H+fxeOT1eiMOAABw8YgqSEaPHq2amhpt27YtfIwcOVLjx48P/71r164qLy8P36a2tlZ1dXUKBAKSpEAgoJqaGjU2NobnlJWVyev1Kjs7+zxdFgAAiCVRvYakd+/eGjp0aMS5nj17qk+fPuHzEyZM0LRp05ScnCyv16spU6YoEAho1KhRkqQxY8YoOztbd911l+bMmaNgMKgZM2aouLhYHo/nPF0WAACIJVG/qPXTPPnkk4qPj1dhYaFaW1uVn5+vZ599NjzepUsXrVq1SpMmTVIgEFDPnj1VVFSkxx577HwvBQAAxIg455yzXkS0QqGQfD6fmpubeT1JZ8nIkPbulfr3l/bssV4NACBGnOljNr/LBgAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJgjSAAAgDmCBAAAmCNIAACAOYIEAACYI0gAAIA5ggQAAJhLsF7A2XDOSZJCoZDxSj5HOjo++pN9BwCcoROP1Sceu08nJoPkgw8+kCRlZmYar+RzaP9+yeezXgUAIMa0tLTI9wmPHzEZJMnJyZKkurq6T7w4fCQUCikzM1P19fXyer3Wy4kJ7Fn02LPosWfRY8+iZ7lnzjm1tLQoPT39E+fFZJDExx9/6YvP5+OHMUper5c9ixJ7Fj32LHrsWfTYs+hZ7dmZPHnAi1oBAIA5ggQAAJiLySDxeDyaNWuWPB6P9VJiBnsWPfYseuxZ9Niz6LFn0YuFPYtzn/Y+HAAAgM9YTD5DAgAALi4ECQAAMEeQAAAAcwQJAAAwR5AAAABzMRkk8+bN06WXXqpu3bopNzdXmzZtsl6SifXr1+vmm29Wenq64uLitGLFiohx55xmzpyptLQ0de/eXXl5edq1a1fEnIMHD2r8+PHyer1KSkrShAkTdOjQoU68is5VWlqqq6++Wr1791ZKSopuvfVW1dbWRsw5cuSIiouL1adPH/Xq1UuFhYVqaGiImFNXV6eCggL16NFDKSkpmj59uo4dO9aZl9Jp5s+fr+HDh4c/4TEQCGj16tXhcfbr082ePVtxcXGaOnVq+Bz7FuknP/mJ4uLiIo7BgweHx9mvU9u7d6++853vqE+fPurevbuGDRumLVu2hMdj6nHAxZilS5e6xMRE9/vf/97t2LHD3XvvvS4pKck1NDRYL63Tvfzyy+7HP/6x++tf/+okueXLl0eMz5492/l8PrdixQr3z3/+033zm990AwcOdB9++GF4zg033OBGjBjhNm7c6P7+97+7yy67zN15552dfCWdJz8/3y1cuNBt377dbdu2zd14440uKyvLHTp0KDxn4sSJLjMz05WXl7stW7a4UaNGuS9/+cvh8WPHjrmhQ4e6vLw8t3XrVvfyyy+7vn37upKSEotL+sy99NJL7m9/+5t75513XG1trfvRj37kunbt6rZv3+6cY78+zaZNm9yll17qhg8f7u6///7wefYt0qxZs9yXvvQlt3///vBx4MCB8Dj7dbKDBw+6AQMGuLvvvttVVVW59957z61du9a9++674Tmx9DgQc0FyzTXXuOLi4vDX7e3tLj093ZWWlhquyt7Hg6Sjo8Olpqa6xx9/PHyuqanJeTwe98c//tE559zOnTudJLd58+bwnNWrV7u4uDi3d+/eTlu7pcbGRifJVVRUOOeO71HXrl3dsmXLwnPeeustJ8lVVlY6546HYHx8vAsGg+E58+fPd16v17W2tnbuBRi55JJL3G9/+1v261O0tLS4QYMGubKyMvd///d/4SBh3042a9YsN2LEiFOOsV+n9tBDD7nrrrvutOOx9jgQU/9k09bWpurqauXl5YXPxcfHKy8vT5WVlYYru/Ds3r1bwWAwYq98Pp9yc3PDe1VZWamkpCSNHDkyPCcvL0/x8fGqqqrq9DVbaG5ulvTRb5Curq7W0aNHI/Zt8ODBysrKiti3YcOGye/3h+fk5+crFAppx44dnbj6ztfe3q6lS5fq8OHDCgQC7NenKC4uVkFBQcT+SPycnc6uXbuUnp6uL3zhCxo/frzq6uoksV+n89JLL2nkyJH69re/rZSUFF155ZV67rnnwuOx9jgQU0Hy/vvvq729PeIHTpL8fr+CwaDRqi5MJ/bjk/YqGAwqJSUlYjwhIUHJycmfi/3s6OjQ1KlTde2112ro0KGSju9JYmKikpKSIuZ+fN9Ota8nxi5GNTU16tWrlzwejyZOnKjly5crOzub/foES5cu1RtvvKHS0tKTxti3k+Xm5mrRokVas2aN5s+fr927d+srX/mKWlpa2K/TeO+99zR//nwNGjRIa9eu1aRJk/SDH/xAixcvlhR7jwMJnXpvwAWkuLhY27dv1+uvv269lAve5Zdfrm3btqm5uVl/+ctfVFRUpIqKCutlXbDq6+t1//33q6ysTN26dbNeTkwYO3Zs+O/Dhw9Xbm6uBgwYoD//+c/q3r274couXB0dHRo5cqR+8YtfSJKuvPJKbd++XQsWLFBRUZHx6qIXU8+Q9O3bV126dDnpldUNDQ1KTU01WtWF6cR+fNJepaamqrGxMWL82LFjOnjw4EW/n5MnT9aqVav06quvKiMjI3w+NTVVbW1tampqipj/8X071b6eGLsYJSYm6rLLLlNOTo5KS0s1YsQIPf300+zXaVRXV6uxsVFXXXWVEhISlJCQoIqKCs2dO1cJCQny+/3s26dISkrSF7/4Rb377rv8nJ1GWlqasrOzI84NGTIk/E9dsfY4EFNBkpiYqJycHJWXl4fPdXR0qLy8XIFAwHBlF56BAwcqNTU1Yq9CoZCqqqrCexUIBNTU1KTq6urwnHXr1qmjo0O5ubmdvubO4JzT5MmTtXz5cq1bt04DBw6MGM/JyVHXrl0j9q22tlZ1dXUR+1ZTUxPxH3FZWZm8Xu9J/3O4WHV0dKi1tZX9Oo3Ro0erpqZG27ZtCx8jR47U+PHjw39n3z7ZoUOH9K9//UtpaWn8nJ3Gtddee9LHFrzzzjsaMGCApBh8HOjUl9CeB0uXLnUej8ctWrTI7dy50913330uKSkp4pXVnxctLS1u69atbuvWrU6Se+KJJ9zWrVvdf/7zH+fc8bd7JSUluRdffNG9+eab7pZbbjnl272uvPJKV1VV5V5//XU3aNCgi/ptv5MmTXI+n8+99tprEW8v/O9//xueM3HiRJeVleXWrVvntmzZ4gKBgAsEAuHxE28vHDNmjNu2bZtbs2aN69ev30X79sKHH37YVVRUuN27d7s333zTPfzwwy4uLs698sorzjn260z977tsnGPfPu7BBx90r732mtu9e7f7xz/+4fLy8lzfvn1dY2Ojc479OpVNmza5hIQE9/Of/9zt2rXLvfDCC65Hjx7u+eefD8+JpceBmAsS55x75plnXFZWlktMTHTXXHON27hxo/WSTLz66qtO0klHUVGRc+74W74eeeQR5/f7ncfjcaNHj3a1tbUR3+ODDz5wd955p+vVq5fzer3unnvucS0tLQZX0zlOtV+S3MKFC8NzPvzwQ/f973/fXXLJJa5Hjx7uW9/6ltu/f3/E9/n3v//txo4d67p37+769u3rHnzwQXf06NFOvprO8b3vfc8NGDDAJSYmun79+rnRo0eHY8Q59utMfTxI2LdI48aNc2lpaS4xMdH179/fjRs3LuLzNNivU1u5cqUbOnSo83g8bvDgwe43v/lNxHgsPQ7EOedc5z4nAwAAECmmXkMCAAAuTgQJAAAwR5AAAABzBAkAADBHkAAAAHMECQAAMEeQAAAAcwQJAAAwR5AAAABzBAkAADBHkAAAAHP/D0xawJ+7k75nAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other methods of predictions below. Not really working so pivioted back to above methods."
      ],
      "metadata": {
        "id": "KJDgT2WqCfZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "\n",
        "# === Directory setup ===\n",
        "base_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels\"\n",
        "\n",
        "train_input_dir = f\"{base_dir}/train_input\"\n",
        "train_input_aug_dir = f\"{base_dir}/train_input_aug\"\n",
        "val_input_dir = f\"{base_dir}/val_input\"\n",
        "val_input_aug_dir = f\"{base_dir}/val_input_aug\"\n",
        "\n",
        "train_target_dir = f\"{base_dir}/train\"\n",
        "val_target_dir = f\"{base_dir}/val\"\n",
        "\n",
        "\n",
        "# === Utility: Load YOLO labels ===\n",
        "def load_labels(path):\n",
        "    \"\"\"Load bounding boxes from a YOLO-format label file.\"\"\"\n",
        "    boxes = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                if len(parts) > 0:\n",
        "                    print(f\"⚠️ Skipped bad line in {path}: {parts}\")\n",
        "                continue\n",
        "            cls, xc, yc, w, h = map(float, parts)\n",
        "            boxes.append([int(cls), xc, yc, w, h])\n",
        "    return (\n",
        "        torch.tensor(boxes, dtype=torch.float32)\n",
        "        if boxes\n",
        "        else torch.zeros((0, 5), dtype=torch.float32)\n",
        "    )\n",
        "\n",
        "\n",
        "# === Encode bounding boxes (excluding frames) ===\n",
        "def encode_objects(boxes, num_classes=12):\n",
        "    \"\"\"\n",
        "    Encode bounding boxes as one-hot + spatial features.\n",
        "    Excludes picture frames (class 11).\n",
        "    \"\"\"\n",
        "    feats = []\n",
        "    num_obj_classes = num_classes - 1  # excluding class 11\n",
        "\n",
        "    for b in boxes:\n",
        "        cls, xc, yc, w, h = b\n",
        "        if int(cls) == 11:\n",
        "            continue  # skip frames\n",
        "        onehot = F.one_hot(torch.tensor(int(cls)), num_classes=num_obj_classes)\n",
        "        feats.append(torch.cat([onehot.float(), torch.tensor([xc, yc, w, h])]))\n",
        "\n",
        "    # Fallback if no non-frame objects\n",
        "    return torch.stack(feats) if feats else torch.zeros((1, num_obj_classes + 4))\n",
        "\n",
        "\n",
        "# === Dataset class for lazy loading ===\n",
        "class FramePlacementDataset(Dataset):\n",
        "    def __init__(self, input_dir, target_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.files = [f for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        target_fname = fname.replace(\"_aug.txt\", \".txt\")\n",
        "        input_path = os.path.join(self.input_dir, fname)\n",
        "        target_path = os.path.join(self.target_dir, target_fname)\n",
        "\n",
        "        X = load_labels(input_path)   # scene without (some) frames\n",
        "        Y = load_labels(target_path)  # full scene with all frames\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "\n",
        "# === Combine normal + augmented datasets (lazy loading) ===\n",
        "train_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(train_input_dir, train_target_dir),\n",
        "    FramePlacementDataset(train_input_aug_dir, train_target_dir)\n",
        "])\n",
        "\n",
        "val_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(val_input_dir, val_target_dir),\n",
        "    FramePlacementDataset(val_input_aug_dir, val_target_dir)\n",
        "])\n",
        "\n",
        "# === Dataloaders ===\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"✅ Dataloaders ready: {len(train_dataset)} train samples, {len(val_dataset)} val samples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFyJOybHjB4Y",
        "outputId": "18be4369-879c-463a-bda1-c57106d8653a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataloaders ready: 2601 train samples, 477 val samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (X, Y) in enumerate(train_loader):\n",
        "    print(f\"Batch {i}: X shape={X.shape}, Y shape={Y.shape}\")\n",
        "    if i == 2: break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqDzmOewkBrb",
        "outputId": "f14aa2a7-ab25-4c88-cb54-187af3320408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0: X shape=torch.Size([1, 9, 5]), Y shape=torch.Size([1, 9, 5])\n",
            "Batch 1: X shape=torch.Size([1, 6, 5]), Y shape=torch.Size([1, 8, 5])\n",
            "Batch 2: X shape=torch.Size([1, 5, 5]), Y shape=torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "# === DETR-style Transformer for Painting Placement ===\n",
        "class DETRPainting(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=2, num_queries=10):\n",
        "        super().__init__()\n",
        "        self.num_queries = num_queries\n",
        "\n",
        "        # Encode object features\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "\n",
        "        # Learnable slot queries\n",
        "        self.query_embed = nn.Parameter(torch.randn(num_queries, d_model))\n",
        "\n",
        "        # Transformer\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Predict presence and bbox\n",
        "        self.pred_class = nn.Linear(d_model, 1)  # presence probability\n",
        "        self.pred_bbox = nn.Linear(d_model, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        enc = self.input_proj(x)  # [B, N, d_model]\n",
        "        queries = self.query_embed.unsqueeze(0).expand(batch_size, -1, -1)  # [B, Q, d_model]\n",
        "        out = self.transformer(enc, queries)\n",
        "        pred_class = torch.sigmoid(self.pred_class(out)).squeeze(-1)\n",
        "        pred_bbox = torch.sigmoid(self.pred_bbox(out))\n",
        "        return pred_class, pred_bbox\n",
        "\n",
        "\n",
        "# === Hungarian Matching ===\n",
        "def hungarian_match(pred_boxes, target_boxes, pred_class, target_presence):\n",
        "    \"\"\"\n",
        "    pred_boxes: [Q,4], pred_class: [Q]\n",
        "    target_boxes: [T,4], target_presence: [T]\n",
        "    Returns matched indices (pred_idx, target_idx)\n",
        "    \"\"\"\n",
        "    if target_boxes.numel() == 0:\n",
        "        return torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)\n",
        "\n",
        "    # Compute cost matrix: L1 + 1-IoU\n",
        "    cost_bbox = torch.cdist(pred_boxes, target_boxes, p=1)\n",
        "    iou_matrix = torchvision.ops.box_iou(xywh_to_xyxy(pred_boxes), xywh_to_xyxy(target_boxes))\n",
        "    cost_iou = 1 - iou_matrix\n",
        "    # Combine costs\n",
        "    cost_matrix = cost_bbox + cost_iou\n",
        "    # Solve Hungarian\n",
        "    pred_idx, tgt_idx = linear_sum_assignment(cost_matrix.detach().cpu())\n",
        "    return torch.tensor(pred_idx, dtype=torch.long), torch.tensor(tgt_idx, dtype=torch.long)\n",
        "\n",
        "\n",
        "# === Loss ===\n",
        "def detr_painting_loss(pred_class, pred_boxes, target_boxes, target_presence):\n",
        "    \"\"\"\n",
        "    pred_class: [B,Q], pred_boxes: [B,Q,4]\n",
        "    target_boxes: list of tensors [Ti,4], target_presence: list of tensors [Ti]\n",
        "    \"\"\"\n",
        "    batch_size, Q, _ = pred_boxes.shape\n",
        "    total_loss = 0.0\n",
        "    for b in range(batch_size):\n",
        "        pb = pred_boxes[b]\n",
        "        pc = pred_class[b]\n",
        "        tb = target_boxes[b]\n",
        "        tp = target_presence[b]\n",
        "        pred_idx, tgt_idx = hungarian_match(pb, tb, pc, tp)\n",
        "\n",
        "        if len(pred_idx) > 0:\n",
        "            # Classification loss for matched slots\n",
        "            cls_loss = F.binary_cross_entropy(pc[pred_idx], tp[tgt_idx])\n",
        "            # Box loss: L1 + GIoU\n",
        "            bbox_l1 = F.l1_loss(pb[pred_idx], tb[tgt_idx])\n",
        "            bbox_giou = 1 - torch.diag(torchvision.ops.generalized_box_iou(\n",
        "                xywh_to_xyxy(pb[pred_idx]),\n",
        "                xywh_to_xyxy(tb[tgt_idx])\n",
        "            )).mean()\n",
        "            total_loss += cls_loss + bbox_l1 + bbox_giou\n",
        "        else:\n",
        "            total_loss += pc.mean()  # penalize false positives\n",
        "\n",
        "    return total_loss / batch_size\n",
        "\n",
        "\n",
        "# === Helper xywh -> xyxy ===\n",
        "def xywh_to_xyxy(boxes):\n",
        "    x, y, w, h = boxes.unbind(-1)\n",
        "    return torch.stack([x - w/2, y - h/2, x + w/2, y + h/2], dim=-1)\n"
      ],
      "metadata": {
        "id": "wIn95j_ClngN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model v3"
      ],
      "metadata": {
        "id": "A1TrCCoTl6Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Directories ===\n",
        "base_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels\"\n",
        "\n",
        "train_input_dir = f\"{base_dir}/train_input\"\n",
        "train_input_aug_dir = f\"{base_dir}/train_input_aug\"\n",
        "val_input_dir = f\"{base_dir}/val_input\"\n",
        "val_input_aug_dir = f\"{base_dir}/val_input_aug\"\n",
        "\n",
        "train_target_dir = f\"{base_dir}/train\"\n",
        "val_target_dir = f\"{base_dir}/val\"\n",
        "\n",
        "# === Load YOLO labels ===\n",
        "def load_labels(path):\n",
        "    boxes = []\n",
        "    if not os.path.exists(path):\n",
        "        return torch.zeros((0,5), dtype=torch.float32)\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            cls, xc, yc, w, h = map(float, parts)\n",
        "            boxes.append([int(cls), xc, yc, w, h])\n",
        "    return torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,5), dtype=torch.float32)\n",
        "\n",
        "# === Dataset ===\n",
        "class FramePlacementDataset(Dataset):\n",
        "    def __init__(self, input_dir, target_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.files = [f for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        target_fname = fname.replace(\"_aug.txt\", \".txt\")\n",
        "        X = load_labels(os.path.join(self.input_dir, fname))\n",
        "        Y = load_labels(os.path.join(self.target_dir, target_fname))\n",
        "        return X, Y\n",
        "\n",
        "# === Encode objects (skipping frames) ===\n",
        "def encode_objects(boxes, num_classes=12):\n",
        "    feats = []\n",
        "    num_obj_classes = num_classes - 1\n",
        "    for b in boxes:\n",
        "        cls, xc, yc, w, h = b\n",
        "        if int(cls) == 11:  # skip frame\n",
        "            continue\n",
        "        onehot = F.one_hot(torch.tensor(int(cls)), num_classes=num_obj_classes)\n",
        "        feats.append(torch.cat([onehot.float(), torch.tensor([xc, yc, w, h])]))\n",
        "    if feats:\n",
        "        return torch.stack(feats)\n",
        "    else:\n",
        "        return None  # <-- return None for zero objects\n",
        "\n",
        "# === IoU Loss ===\n",
        "def iou_loss(pred_boxes, target_boxes, eps=1e-6):\n",
        "    pred_xy1 = pred_boxes[..., :2] - pred_boxes[..., 2:] / 2\n",
        "    pred_xy2 = pred_boxes[..., :2] + pred_boxes[..., 2:] / 2\n",
        "    target_xy1 = target_boxes[..., :2] - target_boxes[..., 2:] / 2\n",
        "    target_xy2 = target_boxes[..., :2] + target_boxes[..., 2:] / 2\n",
        "\n",
        "    inter_xy1 = torch.max(pred_xy1, target_xy1)\n",
        "    inter_xy2 = torch.min(pred_xy2, target_xy2)\n",
        "    inter_wh = (inter_xy2 - inter_xy1).clamp(min=0)\n",
        "    inter_area = inter_wh[..., 0] * inter_wh[..., 1]\n",
        "\n",
        "    pred_area = pred_boxes[..., 2] * pred_boxes[..., 3]\n",
        "    target_area = target_boxes[..., 2] * target_boxes[..., 3]\n",
        "\n",
        "    union_area = pred_area + target_area - inter_area + eps\n",
        "    iou = inter_area / union_area\n",
        "    return 1 - iou.mean()\n",
        "\n",
        "# === Transformer model ===\n",
        "class PaintingsTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_layers, num_classes=12):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.decoder = nn.Linear(d_model, 5)  # [cls_prob, xc, yc, w, h]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.encoder(x)  # batch_first=True\n",
        "        out = self.decoder(x)\n",
        "        return out\n",
        "\n",
        "# === Training loop ===\n",
        "def train_val_loop(model, train_loader, val_loader, epochs, lr, device):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "    criterion = iou_loss  # or nn.MSELoss() for initial testing\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---- Training ----\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, Y in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss = 0\n",
        "            for xi, yi in zip(X, Y):\n",
        "                xi_enc = encode_objects(xi)\n",
        "                yi_enc = encode_objects(yi)\n",
        "                if xi_enc is None or yi_enc is None:\n",
        "                    continue  # skip zero-object scenes\n",
        "                xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                yi_enc = yi_enc.to(device)\n",
        "                pred = model(xi_enc).squeeze(0)\n",
        "                batch_loss += criterion(pred[:, -4:], yi_enc[:, -4:])\n",
        "            if batch_loss > 0:\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += batch_loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # ---- Validation ----\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X, Y in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
        "                batch_loss = 0\n",
        "                for xi, yi in zip(X, Y):\n",
        "                    xi_enc = encode_objects(xi)\n",
        "                    yi_enc = encode_objects(yi)\n",
        "                    if xi_enc is None or yi_enc is None:\n",
        "                        continue\n",
        "                    xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                    yi_enc = yi_enc.to(device)\n",
        "                    pred = model(xi_enc).squeeze(0)\n",
        "                    batch_loss += criterion(pred[:, -4:], yi_enc[:, -4:])\n",
        "                if batch_loss > 0:\n",
        "                    val_loss += batch_loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
        "    return val_loss\n",
        "\n",
        "# === Data Loaders ===\n",
        "train_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(train_input_dir, train_target_dir),\n",
        "    FramePlacementDataset(train_input_aug_dir, train_target_dir)\n",
        "])\n",
        "val_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(val_input_dir, val_target_dir),\n",
        "    FramePlacementDataset(val_input_aug_dir, val_target_dir)\n",
        "])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# === Model & training ===\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = PaintingsTransformer(input_dim=15, d_model=32, nhead=1, num_layers=1).to(device)\n",
        "\n",
        "train_val_loop(model, train_loader, val_loader, epochs=2, lr=1e-3, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDSU-wIqwMU2",
        "outputId": "0ebad13a-afd5-4aa2-f819-5b0413b8bbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]: 100%|██████████| 2601/2601 [11:09<00:00,  3.89it/s]\n",
            "Epoch 1 [Val]: 100%|██████████| 477/477 [06:05<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2: train_loss=0.7348, val_loss=0.6102\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]: 100%|██████████| 2601/2601 [00:55<00:00, 46.93it/s]\n",
            "Epoch 2 [Val]: 100%|██████████| 477/477 [00:06<00:00, 74.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2: train_loss=0.5869, val_loss=0.5214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5214375742326492"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "def show_boxes(preds, targets, img_w=640, img_h=480):\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    preds = preds.detach().cpu()\n",
        "    targets = targets.detach().cpu()\n",
        "\n",
        "    # preds assumed to be [num_objs, 5]: [cls_logit, xc, yc, w, h]\n",
        "    # Convert class logits to class IDs\n",
        "    if preds.shape[1] > 4:\n",
        "        cls_pred = preds[:, 0].argmax(dim=-1)  # if logits for multiple classes\n",
        "        preds = preds[cls_pred==11][:, 1:]    # only class 11\n",
        "    # Else, preds already bbox-only\n",
        "\n",
        "    # Draw predictions\n",
        "    for (x, y, w, h) in preds:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='r', facecolor='none', linewidth=2\n",
        "        ))\n",
        "\n",
        "    # Draw targets\n",
        "    for (x, y, w, h) in targets:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='g', facecolor='none', linewidth=2\n",
        "        ))\n",
        "\n",
        "    ax.set_xlim(0, img_w)\n",
        "    ax.set_ylim(img_h, 0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.show()\n",
        "X, Y = val_dataset[100]  # or any sample\n",
        "X_enc = encode_objects(X)  # this skips class 11\n",
        "preds = model(X_enc.unsqueeze(0).to(device)).squeeze(0)  # [num_objs, 5]\n",
        "\n",
        "# Targets: only class 11\n",
        "targets = Y[Y[:,0]==11][:,1:]  # [xc,yc,w,h]\n",
        "\n",
        "show_boxes(preds, targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "3BKgmVpZyhKk",
        "outputId": "393b1485-7e7f-4ff8-b277-e2cb3f11b1d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#v4"
      ],
      "metadata": {
        "id": "YMJba87d6IcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t5XdvZU47-H7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from tqdm import tqdm\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "# === Directories ===\n",
        "base_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels\"\n",
        "\n",
        "train_input_dir = f\"{base_dir}/train_input\"\n",
        "train_input_aug_dir = f\"{base_dir}/train_input_aug\"\n",
        "val_input_dir = f\"{base_dir}/val_input\"\n",
        "val_input_aug_dir = f\"{base_dir}/val_input_aug\"\n",
        "\n",
        "train_target_dir = f\"{base_dir}/train\"\n",
        "val_target_dir = f\"{base_dir}/val\"\n",
        "\n",
        "# === Load YOLO labels ===\n",
        "def load_labels(path):\n",
        "    boxes = []\n",
        "    if not os.path.exists(path):\n",
        "        return torch.zeros((0,5), dtype=torch.float32)\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            cls, xc, yc, w, h = map(float, parts)\n",
        "            boxes.append([int(cls), xc, yc, w, h])\n",
        "    return torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,5), dtype=torch.float32)\n",
        "\n",
        "# === Dataset ===\n",
        "class FramePlacementDataset(Dataset):\n",
        "    def __init__(self, input_dir, target_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.files = [f for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        target_fname = fname.replace(\"_aug.txt\", \".txt\")\n",
        "        X = load_labels(os.path.join(self.input_dir, fname))\n",
        "        Y = load_labels(os.path.join(self.target_dir, target_fname))\n",
        "        return X, Y\n",
        "\n",
        "# === Encode all objects ===\n",
        "def encode_objects(boxes, num_classes=12):\n",
        "    feats = []\n",
        "    for b in boxes:\n",
        "        cls, xc, yc, w, h = b\n",
        "        onehot = F.one_hot(torch.tensor(int(cls)), num_classes=num_classes)\n",
        "        feats.append(torch.cat([onehot.float(), torch.tensor([xc, yc, w, h])]))\n",
        "    if feats:\n",
        "        return torch.stack(feats)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# === IoU loss for box regression ===\n",
        "def iou_loss(pred_boxes, target_boxes, eps=1e-6):\n",
        "    pred_xy1 = pred_boxes[..., :2] - pred_boxes[..., 2:] / 2\n",
        "    pred_xy2 = pred_boxes[..., :2] + pred_boxes[..., 2:] / 2\n",
        "    target_xy1 = target_boxes[..., :2] - target_boxes[..., 2:] / 2\n",
        "    target_xy2 = target_boxes[..., :2] + target_boxes[..., 2:] / 2\n",
        "\n",
        "    inter_xy1 = torch.max(pred_xy1, target_xy1)\n",
        "    inter_xy2 = torch.min(pred_xy2, target_xy2)\n",
        "    inter_wh = (inter_xy2 - inter_xy1).clamp(min=0)\n",
        "    inter_area = inter_wh[...,0] * inter_wh[...,1]\n",
        "\n",
        "    pred_area = pred_boxes[...,2] * pred_boxes[...,3]\n",
        "    target_area = target_boxes[...,2] * target_boxes[...,3]\n",
        "\n",
        "    union_area = pred_area + target_area - inter_area + eps\n",
        "    iou = inter_area / union_area\n",
        "    return 1 - iou.mean()\n",
        "\n",
        "# === Transformer model ===\n",
        "class PaintingsTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, d_model=32, nhead=1, num_layers=1, num_classes=12):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.cls_head = nn.Linear(d_model, num_classes)   # class logits\n",
        "        self.box_head = nn.Linear(d_model, 4)            # bbox regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.encoder(x)\n",
        "        cls_logits = self.cls_head(x)\n",
        "        bbox_pred = self.box_head(x)\n",
        "        return cls_logits, bbox_pred\n",
        "\n",
        "# === Hungarian matching loss ===\n",
        "def hungarian_loss(pred_cls_logits, pred_boxes, target_boxes, conf_thresh=0.2):\n",
        "    cls_probs = F.softmax(pred_cls_logits, dim=-1)\n",
        "    pred_mask = cls_probs.max(dim=-1).values > conf_thresh\n",
        "    filtered_pred_boxes = pred_boxes[pred_mask]\n",
        "\n",
        "    if target_boxes.shape[0]==0 or filtered_pred_boxes.shape[0]==0:\n",
        "        return torch.tensor(0., device=pred_boxes.device)\n",
        "\n",
        "    cost_matrix = torch.zeros(filtered_pred_boxes.shape[0], target_boxes.shape[0], device=pred_boxes.device)\n",
        "    for i, p in enumerate(filtered_pred_boxes):\n",
        "        for j, t in enumerate(target_boxes):\n",
        "            cost_matrix[i,j] = iou_loss(p.unsqueeze(0), t.unsqueeze(0))\n",
        "\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix.detach().cpu())\n",
        "    matched_pred = filtered_pred_boxes[row_ind]\n",
        "    matched_target = target_boxes[col_ind].to(filtered_pred_boxes.device)\n",
        "    return iou_loss(matched_pred, matched_target)\n",
        "\n",
        "# === Training loop ===\n",
        "def train_val_loop(model, train_loader, val_loader, epochs, lr, device):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---- Training ----\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, Y in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss = 0\n",
        "            for xi, yi in zip(X, Y):\n",
        "                xi_enc = encode_objects(xi)\n",
        "                if xi_enc is None:\n",
        "                    continue\n",
        "                xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                yi_boxes = yi[:,1:].to(device)  # use all targets\n",
        "                cls_logits, bbox_pred = model(xi_enc)\n",
        "                cls_logits = cls_logits.squeeze(0)\n",
        "                bbox_pred = bbox_pred.squeeze(0)\n",
        "                batch_loss += hungarian_loss(cls_logits, bbox_pred, yi_boxes)\n",
        "            if batch_loss > 0:\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += batch_loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        # ---- Validation ----\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X, Y in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
        "                batch_loss = 0\n",
        "                for xi, yi in zip(X, Y):\n",
        "                    xi_enc = encode_objects(xi)\n",
        "                    if xi_enc is None:\n",
        "                        continue\n",
        "                    xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                    yi_boxes = yi[:,1:].to(device)\n",
        "                    cls_logits, bbox_pred = model(xi_enc)\n",
        "                    cls_logits = cls_logits.squeeze(0)\n",
        "                    bbox_pred = bbox_pred.squeeze(0)\n",
        "                    batch_loss += hungarian_loss(cls_logits, bbox_pred, yi_boxes)\n",
        "                val_loss += batch_loss.item() if batch_loss>0 else 0\n",
        "        val_loss /= len(val_loader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}\")\n",
        "\n",
        "# === Visualization ===\n",
        "def show_boxes(cls_logits, bbox_pred, targets, img_w=640, img_h=480, conf_thresh=0.5):\n",
        "    cls_probs = F.softmax(cls_logits, dim=-1)\n",
        "    pred_mask = cls_probs.max(dim=-1).values > conf_thresh\n",
        "    pred_boxes = bbox_pred[pred_mask].detach().cpu()\n",
        "    targets = targets.detach().cpu()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    for (x, y, w, h) in pred_boxes:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='r', facecolor='none', linewidth=2\n",
        "        ))\n",
        "    for (x, y, w, h) in targets[:,1:]:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h, edgecolor='g', facecolor='none', linewidth=2\n",
        "        ))\n",
        "    ax.set_xlim(0, img_w)\n",
        "    ax.set_ylim(img_h, 0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.show()\n",
        "\n",
        "# === Data Loaders ===\n",
        "train_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(train_input_dir, train_target_dir),\n",
        "    FramePlacementDataset(train_input_aug_dir, train_target_dir)\n",
        "])\n",
        "val_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(val_input_dir, val_target_dir),\n",
        "    FramePlacementDataset(val_input_aug_dir, val_target_dir)\n",
        "])\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# === Model & training ===\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = PaintingsTransformer(input_dim=16, d_model=32, nhead=1, num_layers=1).to(device)\n",
        "\n",
        "# === Train ===\n",
        "train_val_loop(model, train_loader, val_loader, epochs=2, lr=1e-3, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1qtyttZHc0r",
        "outputId": "a522bbc0-ab35-4e50-cd42-430082f931c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]: 100%|██████████| 2601/2601 [01:45<00:00, 24.63it/s]\n",
            "Epoch 1 [Val]: 100%|██████████| 477/477 [00:06<00:00, 72.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2: train_loss=0.6636, val_loss=0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]: 100%|██████████| 2601/2601 [01:13<00:00, 35.60it/s]\n",
            "Epoch 2 [Val]: 100%|██████████| 477/477 [00:06<00:00, 73.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2: train_loss=0.7074, val_loss=0.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from tqdm import tqdm\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from contextlib import nullcontext\n",
        "\n",
        "\n",
        "# === Dataset and label loading === (unchanged from before)\n",
        "def load_labels(path):\n",
        "    boxes = []\n",
        "    if not os.path.exists(path):\n",
        "        return torch.zeros((0,5), dtype=torch.float32)\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            cls, xc, yc, w, h = map(float, parts)\n",
        "            boxes.append([int(cls), xc, yc, w, h])\n",
        "    return torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,5), dtype=torch.float32)\n",
        "\n",
        "class FramePlacementDataset(Dataset):\n",
        "    def __init__(self, input_dir, target_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.files = [f for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        target_fname = fname.replace(\"_aug.txt\", \".txt\")\n",
        "        X = load_labels(os.path.join(self.input_dir, fname))\n",
        "        Y = load_labels(os.path.join(self.target_dir, target_fname))\n",
        "        return X, Y\n",
        "\n",
        "def encode_input_objects(boxes, num_classes=12):\n",
        "    feats = []\n",
        "    for b in boxes:\n",
        "        cls, xc, yc, w, h = b\n",
        "        onehot = F.one_hot(torch.tensor(int(cls)), num_classes=num_classes)\n",
        "        feats.append(torch.cat([onehot.float(), torch.tensor([xc, yc, w, h])]))\n",
        "    if feats:\n",
        "        return torch.stack(feats)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def encode_target_class11(boxes):\n",
        "    class11_boxes = boxes[boxes[:, 0] == 11][:, 1:]\n",
        "    if class11_boxes.shape[0] == 0:\n",
        "        return None\n",
        "    return class11_boxes.float()\n",
        "\n",
        "\n",
        "# === Loss functions and model === (unchanged from before)\n",
        "def iou_loss(pred_boxes, target_boxes, eps=1e-6):\n",
        "    pred_xy1 = pred_boxes[:, :2] - pred_boxes[:, 2:] / 2\n",
        "    pred_xy2 = pred_boxes[:, :2] + pred_boxes[:, 2:] / 2\n",
        "    target_xy1 = target_boxes[:, :2] - target_boxes[:, 2:] / 2\n",
        "    target_xy2 = target_boxes[:, :2] + target_boxes[:, 2:] / 2\n",
        "\n",
        "    inter_min = torch.max(pred_xy1, target_xy1)\n",
        "    inter_max = torch.min(pred_xy2, target_xy2)\n",
        "    inter_wh = (inter_max - inter_min).clamp(min=0)\n",
        "    inter_area = inter_wh[:, 0] * inter_wh[:, 1]\n",
        "\n",
        "    pred_area = pred_boxes[:, 2] * pred_boxes[:, 3]\n",
        "    target_area = target_boxes[:, 2] * target_boxes[:, 3]\n",
        "\n",
        "    union_area = pred_area + target_area - inter_area + eps\n",
        "    iou = inter_area / union_area\n",
        "    return 1 - iou.mean()\n",
        "\n",
        "def combined_hungarian_loss(pred_boxes, target_boxes, l1_weight=10.0):\n",
        "    if target_boxes is None or pred_boxes.shape[0] == 0:\n",
        "        return torch.tensor(0., device=pred_boxes.device, requires_grad=True)\n",
        "    cost_matrix = torch.zeros(pred_boxes.shape[0], target_boxes.shape[0], device=pred_boxes.device)\n",
        "    for i, p in enumerate(pred_boxes):\n",
        "        for j, t in enumerate(target_boxes):\n",
        "            iou_comp = 1 - (1 - iou_loss(p.unsqueeze(0), t.unsqueeze(0)))  # IoU itself\n",
        "            l1_comp = F.l1_loss(p, t, reduction='mean')\n",
        "            cost_matrix[i, j] = (1 - iou_comp) + l1_weight * l1_comp\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix.detach().cpu().numpy())\n",
        "    matched_pred = pred_boxes[row_ind]\n",
        "    matched_target = target_boxes[col_ind].to(pred_boxes.device)\n",
        "    return iou_loss(matched_pred, matched_target) + l1_weight * F.l1_loss(matched_pred, matched_target)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "class PaintingsTransformer(nn.Module):\n",
        "    def __init__(self, input_dim=16, d_model=64, nhead=4, num_layers=4, dropout=0.2, max_len=100):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.box_head = nn.Linear(d_model, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.encoder(x)\n",
        "        bbox_pred = self.box_head(x)\n",
        "        return bbox_pred\n",
        "\n",
        "# === Metrics calculation functions ===\n",
        "def box_iou(box1, box2, eps=1e-7):\n",
        "    box1_xy1 = box1[:, :2] - box1[:, 2:] / 2\n",
        "    box1_xy2 = box1[:, :2] + box1[:, 2:] / 2\n",
        "    box2_xy1 = box2[:, :2] - box2[:, 2:] / 2\n",
        "    box2_xy2 = box2[:, :2] + box2[:, 2:] / 2\n",
        "\n",
        "    inter_xy1 = torch.max(box1_xy1[:, None, :], box2_xy1[None, :, :])\n",
        "    inter_xy2 = torch.min(box1_xy2[:, None, :], box2_xy2[None, :, :])\n",
        "    inter_wh = (inter_xy2 - inter_xy1).clamp(min=0)\n",
        "    inter_area = inter_wh[:, :, 0] * inter_wh[:, :, 1]\n",
        "\n",
        "    box1_area = box1[:, 2] * box1[:, 3]\n",
        "    box2_area = box2[:, 2] * box2[:, 3]\n",
        "\n",
        "    union_area = box1_area[:, None] + box2_area[None, :] - inter_area + eps\n",
        "    iou = inter_area / union_area\n",
        "    return iou\n",
        "\n",
        "def precision_recall_f1(pred_boxes, target_boxes, iou_thresh=0.5):\n",
        "    if pred_boxes is None or target_boxes is None or (pred_boxes.shape[0] == 0 and target_boxes.shape[0] == 0):\n",
        "        return 1.0, 1.0, 1.0\n",
        "    elif pred_boxes is None or pred_boxes.shape[0] == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    elif target_boxes is None or target_boxes.shape[0] == 0:\n",
        "        return 0.0, 1.0, 0.0\n",
        "\n",
        "    ious = box_iou(pred_boxes, target_boxes)  # (num_pred, num_target)\n",
        "    matches = ious > iou_thresh\n",
        "\n",
        "    true_positives = matches.any(dim=1).sum().item()\n",
        "    false_positives = (matches.any(dim=1) == 0).sum().item()\n",
        "    false_negatives = (matches.any(dim=0) == 0).sum().item()\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives + 1e-7)\n",
        "    recall = true_positives / (true_positives + false_negatives + 1e-7)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-7)\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "# === LR Warmup scheduler helper ===\n",
        "def get_warmup_scheduler(optimizer, warmup_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step >= warmup_steps:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "    return LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "\n",
        "# === Training + validation with metrics and early stopping ===\n",
        "def train_val_loop(model, train_loader, val_loader, epochs, lr, warmup_steps, device):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    warmup_scheduler = get_warmup_scheduler(optimizer, warmup_steps)\n",
        "    plateau_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "    model.to(device)\n",
        "\n",
        "    scaler = GradScaler() if torch.cuda.is_available() else None\n",
        "    autocast_context = autocast if torch.cuda.is_available() else nullcontext\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    patience = 3\n",
        "    trigger_times = 0\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, Y in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss = 0\n",
        "            for xi, yi in zip(X, Y):\n",
        "                xi_enc = encode_input_objects(xi)\n",
        "                if xi_enc is None:\n",
        "                    continue\n",
        "                xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                yi_enc = encode_target_class11(yi)\n",
        "                if yi_enc is None:\n",
        "                    continue\n",
        "                yi_enc = yi_enc.to(device)\n",
        "                with autocast_context():\n",
        "                    bbox_pred = model(xi_enc).squeeze(0)\n",
        "                    loss = combined_hungarian_loss(bbox_pred, yi_enc)\n",
        "                if loss > 0:\n",
        "                    if scaler is not None:\n",
        "                        scaler.scale(loss).backward()\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                    else:\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    batch_loss += loss.item()\n",
        "                else:\n",
        "                    batch_loss += loss.item()\n",
        "                if global_step < warmup_steps:\n",
        "                    warmup_scheduler.step()\n",
        "                global_step += 1\n",
        "            train_loss += batch_loss\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_precisions, all_recalls, all_f1s = [], [], []\n",
        "        with torch.no_grad():\n",
        "            for X, Y in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
        "                batch_loss = 0\n",
        "                for xi, yi in zip(X, Y):\n",
        "                    xi_enc = encode_input_objects(xi)\n",
        "                    if xi_enc is None:\n",
        "                        continue\n",
        "                    xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                    yi_enc = encode_target_class11(yi)\n",
        "                    if yi_enc is None:\n",
        "                        continue\n",
        "                    yi_enc = yi_enc.to(device)\n",
        "                    with autocast_context():\n",
        "                        bbox_pred = model(xi_enc).squeeze(0)\n",
        "                        loss = combined_hungarian_loss(bbox_pred, yi_enc)\n",
        "                    batch_loss += loss.item()\n",
        "\n",
        "                    # Calculate metrics (clamp boxes to [0,1] for IoU)\n",
        "                    pred_boxes = bbox_pred.clamp(0, 1).cpu()\n",
        "                    target_boxes = yi_enc.cpu()\n",
        "                    prec, rec, f1 = precision_recall_f1(pred_boxes, target_boxes)\n",
        "                    all_precisions.append(prec)\n",
        "                    all_recalls.append(rec)\n",
        "                    all_f1s.append(f1)\n",
        "                val_loss += batch_loss\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        mean_prec = sum(all_precisions) / len(all_precisions) if all_precisions else 0\n",
        "        mean_rec = sum(all_recalls) / len(all_recalls) if all_recalls else 0\n",
        "        mean_f1 = sum(all_f1s) / len(all_f1s) if all_f1s else 0\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}, \"\n",
        "              f\"val_Precision={mean_prec:.4f}, val_Recall={mean_rec:.4f}, val_F1={mean_f1:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            trigger_times = 0\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # LR scheduler step after warmup\n",
        "        if global_step >= warmup_steps:\n",
        "            plateau_scheduler.step(val_loss)\n",
        "\n",
        "    # Load best model weights after training completes/early stopping\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "# === DataLoader setup ===\n",
        "base_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels\"\n",
        "train_input_dir = f\"{base_dir}/train_input\"\n",
        "train_input_aug_dir = f\"{base_dir}/train_input_aug\"\n",
        "val_input_dir = f\"{base_dir}/val_input\"\n",
        "val_input_aug_dir = f\"{base_dir}/val_input_aug\"\n",
        "train_target_dir = f\"{base_dir}/train\"\n",
        "val_target_dir = f\"{base_dir}/val\"\n",
        "\n",
        "num_workers = 2\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "train_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(train_input_dir, train_target_dir),\n",
        "    FramePlacementDataset(train_input_aug_dir, train_target_dir)\n",
        "])\n",
        "val_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(val_input_dir, val_target_dir),\n",
        "    FramePlacementDataset(val_input_aug_dir, val_target_dir)\n",
        "])\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = PaintingsTransformer(input_dim=16, d_model=64, nhead=4, num_layers=4, dropout=0.2).to(device)\n",
        "\n",
        "warmup_steps = 1000\n",
        "\n",
        "# Start training\n",
        "train_val_loop(model, train_loader, val_loader, epochs=10, lr=1e-3, warmup_steps=warmup_steps, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xi1j9Rwbaa8",
        "outputId": "d203f1c9-8880-4e39-d640-a3979be981d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]: 100%|██████████| 2601/2601 [01:35<00:00, 27.14it/s]\n",
            "Epoch 1 [Val]: 100%|██████████| 477/477 [00:10<00:00, 45.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10: train_loss=1.166032, val_loss=1.409517, val_Precision=0.0064, val_Recall=0.0068, val_F1=0.0064\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]: 100%|██████████| 2601/2601 [01:35<00:00, 27.14it/s]\n",
            "Epoch 2 [Val]: 100%|██████████| 477/477 [00:10<00:00, 43.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10: train_loss=1.024541, val_loss=1.256212, val_Precision=0.0106, val_Recall=0.0070, val_F1=0.0084\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Train]: 100%|██████████| 2601/2601 [01:40<00:00, 25.93it/s]\n",
            "Epoch 3 [Val]: 100%|██████████| 477/477 [00:10<00:00, 44.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10: train_loss=1.013722, val_loss=1.297500, val_Precision=0.0088, val_Recall=0.0077, val_F1=0.0073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Train]: 100%|██████████| 2601/2601 [01:36<00:00, 27.08it/s]\n",
            "Epoch 4 [Val]: 100%|██████████| 477/477 [00:10<00:00, 45.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10: train_loss=1.006586, val_loss=1.425765, val_Precision=0.0141, val_Recall=0.0103, val_F1=0.0119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5 [Train]: 100%|██████████| 2601/2601 [01:37<00:00, 26.58it/s]\n",
            "Epoch 5 [Val]: 100%|██████████| 477/477 [00:09<00:00, 49.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10: train_loss=1.009543, val_loss=1.325693, val_Precision=0.0141, val_Recall=0.0115, val_F1=0.0125\n",
            "Early stopping at epoch 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_boxes(bbox_pred, targets, img_w=640, img_h=480):\n",
        "    \"\"\"\n",
        "    bbox_pred: [num_pred, 4] - predicted boxes (xc, yc, w, h, normalized)\n",
        "    targets: [num_target, 5] - target boxes (cls, xc, yc, w, h)\n",
        "    \"\"\"\n",
        "\n",
        "    pred_boxes = bbox_pred.detach().cpu()\n",
        "    target_boxes = targets[targets[:, 0] == 11][:, 1:].detach().cpu()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Red = predicted boxes (all assumed class 11)\n",
        "    for (x, y, w, h) in pred_boxes:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h,\n",
        "            edgecolor='r', facecolor='none', linewidth=2\n",
        "        ))\n",
        "\n",
        "    # Green = ground truth boxes class 11\n",
        "    for (x, y, w, h) in target_boxes:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h,\n",
        "            edgecolor='g', facecolor='none', linewidth=2\n",
        "        ))\n",
        "\n",
        "    ax.set_xlim(0, img_w)\n",
        "    ax.set_ylim(img_h, 0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "X, Y = val_dataset[14]\n",
        "X_enc = encode_input_objects(X)\n",
        "if X_enc is not None:\n",
        "    bbox_pred = model(X_enc.unsqueeze(0).to(device)).squeeze(0)\n",
        "    show_boxes(bbox_pred, Y, img_w=640, img_h=480)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "1RCx1SsdRV2L",
        "outputId": "834470ce-00ef-458c-c4d6-8429c9ef5142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_boxes(cls_logits, bbox_pred, targets, img_w=640, img_h=480, conf_thresh=0.1):\n",
        "    \"\"\"\n",
        "    cls_logits: [num_pred, 12] - class logits\n",
        "    bbox_pred: [num_pred, 4] - predicted boxes (xc, yc, w, h, normalized)\n",
        "    targets: [num_target, 5] - target boxes (cls, xc, yc, w, h)\n",
        "    \"\"\"\n",
        "    cls_probs = F.softmax(cls_logits, dim=-1)\n",
        "\n",
        "    # Only keep predictions where class 11 probability > conf_thresh\n",
        "    pred_mask = cls_probs[:, 11] > conf_thresh\n",
        "    pred_boxes = bbox_pred[pred_mask].detach().cpu()\n",
        "\n",
        "    # Keep only target boxes of class 11\n",
        "    target_boxes = targets[targets[:,0] == 11][:, 1:].detach().cpu()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Red = predictions\n",
        "    for (x, y, w, h) in pred_boxes:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h,\n",
        "            edgecolor='r', facecolor='none', linewidth=2\n",
        "        ))\n",
        "\n",
        "    # Green = targets\n",
        "    for (x, y, w, h) in target_boxes:\n",
        "        ax.add_patch(patches.Rectangle(\n",
        "            ((x - w/2) * img_w, (y - h/2) * img_h),\n",
        "            w * img_w, h * img_h,\n",
        "            edgecolor='g', facecolor='none', linewidth=2\n",
        "        ))\n",
        "\n",
        "    ax.set_xlim(0, img_w)\n",
        "    ax.set_ylim(img_h, 0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "F36lvrraA-im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = val_dataset[20]\n",
        "X_enc = encode_objects(X)\n",
        "if X_enc is not None:\n",
        "    cls_logits, bbox_pred = model(X_enc.unsqueeze(0).to(device))\n",
        "    cls_logits = cls_logits.squeeze(0)\n",
        "    bbox_pred = bbox_pred.squeeze(0)\n",
        "    show_boxes(cls_logits, bbox_pred, Y, conf_thresh=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gtfIzzMqBANJ",
        "outputId": "ee9fdbd2-c4c0-4de2-a9b2-4d5a8c7e1e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2291515925.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_enc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_enc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcls_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_enc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcls_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbbox_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fixed code with everything that was deleted restored (I hope)----------------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "_KOriUm6SsHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FIXED CODE HERE"
      ],
      "metadata": {
        "id": "CV-imyCZTppb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, WeightedRandomSampler\n",
        "import itertools\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from contextlib import nullcontext\n",
        "import matplotlib.patches as patches\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "# === Dataset and label loading ===\n",
        "\n",
        "def load_labels(path):\n",
        "    boxes = []\n",
        "    if not os.path.exists(path):\n",
        "        return torch.zeros((0,5), dtype=torch.float32)\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            cls, xc, yc, w, h = map(float, parts)\n",
        "            boxes.append([int(cls), xc, yc, w, h])\n",
        "    return torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,5), dtype=torch.float32)\n",
        "\n",
        "class FramePlacementDataset(Dataset):\n",
        "    def __init__(self, input_dir, target_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.files = [f for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        target_fname = fname.replace(\"_aug.txt\", \".txt\")\n",
        "        X = load_labels(os.path.join(self.input_dir, fname))\n",
        "        Y = load_labels(os.path.join(self.target_dir, target_fname))\n",
        "        return X, Y\n",
        "\n",
        "def encode_input_objects(boxes, num_classes=12):\n",
        "    feats = []\n",
        "    for b in boxes:\n",
        "        cls, xc, yc, w, h = b\n",
        "        onehot = F.one_hot(torch.tensor(int(cls)), num_classes=num_classes)\n",
        "        feats.append(torch.cat([onehot.float(), torch.tensor([xc, yc, w, h])]))\n",
        "    if feats:\n",
        "        return torch.stack(feats)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def encode_target_class11(boxes):\n",
        "    class11_boxes = boxes[boxes[:, 0] == 11][:, 1:]\n",
        "    if class11_boxes.shape[0] == 0:\n",
        "        return None\n",
        "    return class11_boxes.float()\n",
        "\n",
        "# === Loss functions ===\n",
        "\n",
        "def iou_loss(pred_boxes, target_boxes, eps=1e-6):\n",
        "    pred_xy1 = pred_boxes[:, :2] - pred_boxes[:, 2:] / 2\n",
        "    pred_xy2 = pred_boxes[:, :2] + pred_boxes[:, 2:] / 2\n",
        "    target_xy1 = target_boxes[:, :2] - target_boxes[:, 2:] / 2\n",
        "    target_xy2 = target_boxes[:, :2] + target_boxes[:, 2:] / 2\n",
        "\n",
        "    inter_min = torch.max(pred_xy1, target_xy1)\n",
        "    inter_max = torch.min(pred_xy2, target_xy2)\n",
        "    inter_wh = (inter_max - inter_min).clamp(min=0)\n",
        "    inter_area = inter_wh[:, 0] * inter_wh[:, 1]\n",
        "\n",
        "    pred_area = pred_boxes[:, 2] * pred_boxes[:, 3]\n",
        "    target_area = target_boxes[:, 2] * target_boxes[:, 3]\n",
        "\n",
        "    union_area = pred_area + target_area - inter_area + eps\n",
        "    iou = inter_area / union_area\n",
        "    return 1 - iou.mean()\n",
        "\n",
        "def combined_hungarian_loss(pred_boxes, target_boxes, l1_weight=10.0):\n",
        "    \"\"\"\n",
        "    pred_boxes: Tensor of shape (N_pred, 4)\n",
        "    target_boxes: Tensor of shape (N_target, 4)\n",
        "\n",
        "    Computes Hungarian matching loss combining IoU and L1 loss.\n",
        "    This version vectorizes cost matrix computation using torch.cdist\n",
        "    and performs optimal matching with scipy's linear_sum_assignment.\n",
        "    \"\"\"\n",
        "    if target_boxes is None or pred_boxes.shape[0] == 0:\n",
        "        return torch.tensor(0., device=pred_boxes.device, requires_grad=True)\n",
        "\n",
        "    # Calculate L1 cost matrix (shape: N_pred x N_target)\n",
        "    l1_cost = torch.cdist(pred_boxes, target_boxes, p=1)  # pairwise L1 distance\n",
        "\n",
        "    # Calculate IoU cost matrix\n",
        "    # For vectorized IoU, compute pairwise IoU between all pred and target boxes\n",
        "    pred_xy1 = pred_boxes[:, :2] - pred_boxes[:, 2:] / 2\n",
        "    pred_xy2 = pred_boxes[:, :2] + pred_boxes[:, 2:] / 2\n",
        "    target_xy1 = target_boxes[:, :2] - target_boxes[:, 2:] / 2\n",
        "    target_xy2 = target_boxes[:, :2] + target_boxes[:, 2:] / 2\n",
        "\n",
        "    inter_min = torch.max(pred_xy1[:, None, :], target_xy1[None, :, :])\n",
        "    inter_max = torch.min(pred_xy2[:, None, :], target_xy2[None, :, :])\n",
        "    inter_wh = (inter_max - inter_min).clamp(min=0)  # shape (N_pred x N_target x 2)\n",
        "    inter_area = inter_wh[..., 0] * inter_wh[..., 1]\n",
        "\n",
        "    pred_area = (pred_boxes[:, 2] * pred_boxes[:, 3])[:, None]  # (N_pred, 1)\n",
        "    target_area = (target_boxes[:, 2] * target_boxes[:, 3])[None, :]  # (1, N_target)\n",
        "\n",
        "    union_area = pred_area + target_area - inter_area + 1e-6\n",
        "    iou_cost = 1 - (inter_area / union_area)  # IoU cost matrix\n",
        "\n",
        "    # Combine costs: IoU + weighted L1\n",
        "    cost_matrix = iou_cost + l1_weight * l1_cost\n",
        "\n",
        "    # Solve assignment using Hungarian algorithm (in numpy)\n",
        "    cost_cpu = cost_matrix.detach().cpu().numpy()\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_cpu)\n",
        "\n",
        "    matched_pred = pred_boxes[row_ind]\n",
        "    matched_target = target_boxes[col_ind].to(pred_boxes.device)\n",
        "\n",
        "    # Compute final loss: IoU loss + weighted L1 loss on matched pairs\n",
        "    loss = (iou_cost[row_ind, col_ind]).mean() + l1_weight * F.l1_loss(matched_pred, matched_target)\n",
        "\n",
        "    return loss\n",
        "\n",
        "# === Positional Encoding ===\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# === PaintingsTransformer Model ===\n",
        "\n",
        "class PaintingsTransformer(nn.Module):\n",
        "    def __init__(self, input_dim=16, d_model=64, nhead=4, num_layers=4, dropout=0.2, max_len=100):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.box_head = nn.Linear(d_model, 4)\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.encoder(x)\n",
        "        bbox_pred = self.box_head(x)\n",
        "        return bbox_pred\n",
        "\n",
        "# === Training/validation loop (with warmup, early stop, mixed precision) ===\n",
        "\n",
        "def get_warmup_scheduler(optimizer, warmup_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step >= warmup_steps:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "    return LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "def train_val_loop(model, train_loader, val_loader, epochs, lr, warmup_steps, device):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    warmup_scheduler = get_warmup_scheduler(optimizer, warmup_steps)\n",
        "    plateau_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "    model.to(device)\n",
        "\n",
        "    scaler = GradScaler() if torch.cuda.is_available() else None\n",
        "    autocast_context = autocast if torch.cuda.is_available() else nullcontext\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    patience = 3\n",
        "    trigger_times = 0\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, Y in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss = 0\n",
        "            for xi, yi in zip(X, Y):\n",
        "                xi_enc = encode_input_objects(xi)\n",
        "                if xi_enc is None:\n",
        "                    continue\n",
        "                xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                yi_enc = encode_target_class11(yi)\n",
        "                if yi_enc is None:\n",
        "                    continue\n",
        "                yi_enc = yi_enc.to(device)\n",
        "                with autocast_context():\n",
        "                    bbox_pred = model(xi_enc).squeeze(0)\n",
        "                    loss = combined_hungarian_loss(bbox_pred, yi_enc)\n",
        "                if loss > 0:\n",
        "                    if scaler is not None:\n",
        "                        scaler.scale(loss).backward()\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                    else:\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    batch_loss += loss.item()\n",
        "                else:\n",
        "                    batch_loss += loss.item()\n",
        "                if global_step < warmup_steps:\n",
        "                    warmup_scheduler.step()\n",
        "                global_step += 1\n",
        "            train_loss += batch_loss\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X, Y in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
        "                batch_loss = 0\n",
        "                for xi, yi in zip(X, Y):\n",
        "                    xi_enc = encode_input_objects(xi)\n",
        "                    if xi_enc is None:\n",
        "                        continue\n",
        "                    xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                    yi_enc = encode_target_class11(yi)\n",
        "                    if yi_enc is None:\n",
        "                        continue\n",
        "                    yi_enc = yi_enc.to(device)\n",
        "                    with autocast_context():\n",
        "                        bbox_pred = model(xi_enc).squeeze(0)\n",
        "                        loss = combined_hungarian_loss(bbox_pred, yi_enc)\n",
        "                    batch_loss += loss.item()\n",
        "                val_loss += batch_loss\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            trigger_times = 0\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        if global_step >= warmup_steps:\n",
        "            plateau_scheduler.step(val_loss)\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "# === Hyperparameter grid search ===\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def evaluate_val_loss(model, val_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for X, Y in val_loader:\n",
        "            for xi, yi in zip(X, Y):\n",
        "                xi_enc = encode_input_objects(xi)\n",
        "                if xi_enc is None:\n",
        "                    continue\n",
        "                xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                yi_enc = encode_target_class11(yi)\n",
        "                if yi_enc is None:\n",
        "                    continue\n",
        "                yi_enc = yi_enc.to(device)\n",
        "                bbox_pred = model(xi_enc).squeeze(0)\n",
        "                loss = combined_hungarian_loss(bbox_pred, yi_enc)\n",
        "                total_loss += loss.item()\n",
        "                count += 1\n",
        "    return total_loss / max(count,1)\n",
        "\n",
        "def grid_search(param_grid, train_dataset, val_dataset, device, epochs=3, warmup_steps=100, log_csv_path=\"grid_search_log.csv\"):\n",
        "    keys = list(param_grid.keys())\n",
        "    best_val_loss = float('inf')\n",
        "    best_params = None\n",
        "    best_model_wts = None\n",
        "\n",
        "    def has_class_11(sample):\n",
        "        X, Y = sample\n",
        "        return (Y[:,0] == 11).any().item()\n",
        "\n",
        "    sample_weights = []\n",
        "    for i in range(len(train_dataset)):\n",
        "        sample = train_dataset[i]\n",
        "        sample_weights.append(5.0 if has_class_11(sample) else 1.0) # Oversample class 11 by 5x\n",
        "\n",
        "    sample_weights = torch.DoubleTensor(sample_weights)\n",
        "    train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    with open(log_csv_path, mode='w', newline='') as log_file:\n",
        "        writer = csv.writer(log_file)\n",
        "        writer.writerow(keys + [\"val_loss\"])\n",
        "\n",
        "        for values in itertools.product(*param_grid.values()):\n",
        "            params = dict(zip(keys, values))\n",
        "            print(f\"\\nTesting params: {params}\")\n",
        "\n",
        "            train_loader = DataLoader(train_dataset,\n",
        "                                      batch_size=params['batch_size'],\n",
        "                                      sampler=train_sampler,\n",
        "                                      num_workers=4,\n",
        "                                      pin_memory=torch.cuda.is_available(),\n",
        "                                      collate_fn=collate_fn)\n",
        "            val_loader = DataLoader(val_dataset,\n",
        "                                    batch_size=params['batch_size'],\n",
        "                                    shuffle=False,\n",
        "                                    num_workers=4,\n",
        "                                    pin_memory=torch.cuda.is_available(),\n",
        "                                    collate_fn=collate_fn)\n",
        "            model = PaintingsTransformer(input_dim=16, d_model=params['d_model'],\n",
        "                                         nhead=4, num_layers=params['num_layers'], dropout=0.2).to(device)\n",
        "            train_val_loop(model, train_loader, val_loader, epochs=epochs, lr=params['lr'], warmup_steps=warmup_steps, device=device)\n",
        "            val_loss = evaluate_val_loss(model, val_loader, device)\n",
        "            print(f\"Validation loss: {val_loss:.6f}\")\n",
        "            writer.writerow(list(values) + [val_loss])\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_params = params\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(\"\\nBest hyperparameters found:\", best_params)\n",
        "    torch.save(best_model_wts, \"best_model.pth\")\n",
        "    print(\"Best model saved as 'best_model.pth'\")\n",
        "    return best_params, best_model_wts\n",
        "\n",
        "# === Dataset and DataLoader Setup ===\n",
        "\n",
        "base_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels\"\n",
        "train_input_dir = f\"{base_dir}/train_input\"\n",
        "train_input_aug_dir = f\"{base_dir}/train_input_aug\"\n",
        "val_input_dir = f\"{base_dir}/val_input\"\n",
        "val_input_aug_dir = f\"{base_dir}/val_input_aug\"\n",
        "train_target_dir = f\"{base_dir}/train\"\n",
        "val_target_dir = f\"{base_dir}/val\"\n",
        "\n",
        "num_workers = 4\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "train_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(train_input_dir, train_target_dir),\n",
        "    FramePlacementDataset(train_input_aug_dir, train_target_dir)\n",
        "])\n",
        "val_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(val_input_dir, val_target_dir),\n",
        "    FramePlacementDataset(val_input_aug_dir, val_target_dir)\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "param_grid = {\n",
        "    'lr': [1e-4, 1e-3],\n",
        "    'batch_size': [1, 4],\n",
        "    'num_layers': [3, 4],\n",
        "    'd_model': [64, 128]\n",
        "}\n",
        "\n",
        "# Run grid search to find best hyperparameters\n",
        "best_params, best_weights = grid_search(param_grid, train_dataset, val_dataset, device, epochs=5, warmup_steps=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "0t5y3ovpTjpb",
        "outputId": "9e2da49b-2983-4ba8-d992-456af70f025a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-392781965.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;31m# Run grid search to find best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-392781965.py\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(param_grid, train_dataset, val_dataset, device, epochs, warmup_steps, log_csv_path)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0msample_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0msample_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_class_11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Oversample class 11 by 5x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-392781965.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mtarget_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_aug.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-392781965.py\u001b[0m in \u001b[0;36mload_labels\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skipping param search to run with best ones"
      ],
      "metadata": {
        "id": "Y8Hzon79T8MV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "\n",
        "# --- Dataset class as per your setup ---\n",
        "\n",
        "class FramePlacementDataset(Dataset):\n",
        "    def __init__(self, input_dir, target_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.files = [f for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        target_fname = fname.replace(\"_aug.txt\", \".txt\")\n",
        "        X = self.load_labels(os.path.join(self.input_dir, fname))\n",
        "        Y = self.load_labels(os.path.join(self.target_dir, target_fname))\n",
        "        return X, Y\n",
        "\n",
        "    @staticmethod\n",
        "    def load_labels(path):\n",
        "        boxes = []\n",
        "        if not os.path.exists(path):\n",
        "            return torch.zeros((0, 5), dtype=torch.float32)\n",
        "        with open(path, \"r\") as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if len(parts) != 5:\n",
        "                    continue\n",
        "                cls, xc, yc, w, h = map(float, parts)\n",
        "                boxes.append([int(cls), xc, yc, w, h])\n",
        "        return torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,5), dtype=torch.float32)\n",
        "\n",
        "# --- Paths (replace with your dataset paths) ---\n",
        "\n",
        "base_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels\"\n",
        "train_input_dir = f\"{base_dir}/train_input\"\n",
        "train_input_aug_dir = f\"{base_dir}/train_input_aug\"\n",
        "train_target_dir = f\"{base_dir}/train\"\n",
        "\n",
        "# --- Create train dataset by concatenating normal and augmented ---\n",
        "\n",
        "train_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(train_input_dir, train_target_dir),\n",
        "    FramePlacementDataset(train_input_aug_dir, train_target_dir)\n",
        "])\n",
        "\n",
        "# --- Custom collate_fn to handle variable length tensors ---\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # batch is list of tuples (X, Y)\n",
        "    # Return as tuple of lists, no stacking to avoid size mismatch\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# --- Worker number finder ---\n",
        "\n",
        "def find_best_num_workers(dataset, batch_size=1, max_workers=8):\n",
        "    results = {}\n",
        "    max_test_workers = min(max_workers, os.cpu_count() or 4)\n",
        "    print(f\"Testing num_workers in [0, ..., {max_test_workers}]\")\n",
        "\n",
        "    for workers in range(max_test_workers + 1):\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False,\n",
        "                            num_workers=workers, pin_memory=torch.cuda.is_available(),\n",
        "                            collate_fn=collate_fn)\n",
        "        start = time.time()\n",
        "        # Iterate once over full loader to time\n",
        "        for _ in loader:\n",
        "            pass\n",
        "        elapsed = time.time() - start\n",
        "        results[workers] = elapsed\n",
        "        print(f\"num_workers={workers} took {elapsed:.2f} seconds\")\n",
        "\n",
        "    best_workers = min(results, key=results.get)\n",
        "    print(f\"Best num_workers: {best_workers} with time {results[best_workers]:.2f}s\")\n",
        "    return best_workers\n",
        "\n",
        "# --- Example usage ---\n",
        "\n",
        "batch_size = 4\n",
        "best_num_workers = find_best_num_workers(train_dataset, batch_size=batch_size, max_workers=8)\n",
        "print(f\"Use num_workers={best_num_workers} in your DataLoader for optimal performance.\")\n",
        "\n",
        "# --- Create final DataLoader with best num_workers ---\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                          num_workers=best_num_workers, pin_memory=torch.cuda.is_available(),\n",
        "                          collate_fn=collate_fn)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "w9EHe1qxX678",
        "outputId": "d00f0528-3b12-425e-bf9d-fbec6a113619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing num_workers in [0, ..., 2]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-496697709.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mbest_num_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_num_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Use num_workers={best_num_workers} in your DataLoader for optimal performance.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-496697709.py\u001b[0m in \u001b[0;36mfind_best_num_workers\u001b[0;34m(dataset, batch_size, max_workers)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Iterate once over full loader to time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 790\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    791\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-496697709.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mtarget_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_aug.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-496697709.py\u001b[0m in \u001b[0;36mload_labels\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, WeightedRandomSampler\n",
        "import itertools\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from contextlib import nullcontext\n",
        "import matplotlib.patches as patches\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "# === Dataset and label loading ===\n",
        "\n",
        "def load_labels(path):\n",
        "    boxes = []\n",
        "    if not os.path.exists(path):\n",
        "        return torch.zeros((0,5), dtype=torch.float32)\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            if len(parts) != 5:\n",
        "                continue\n",
        "            cls, xc, yc, w, h = map(float, parts)\n",
        "            boxes.append([int(cls), xc, yc, w, h])\n",
        "    return torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,5), dtype=torch.float32)\n",
        "\n",
        "class FramePlacementDataset(Dataset):\n",
        "    def __init__(self, input_dir, target_dir):\n",
        "        self.input_dir = input_dir\n",
        "        self.target_dir = target_dir\n",
        "        self.files = [f for f in os.listdir(input_dir) if f.endswith(\".txt\")]\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    def __getitem__(self, idx):\n",
        "        fname = self.files[idx]\n",
        "        target_fname = fname.replace(\"_aug.txt\", \".txt\")\n",
        "        X = load_labels(os.path.join(self.input_dir, fname))\n",
        "        Y = load_labels(os.path.join(self.target_dir, target_fname))\n",
        "        return X, Y\n",
        "\n",
        "def encode_input_objects(boxes, num_classes=12):\n",
        "    feats = []\n",
        "    for b in boxes:\n",
        "        cls, xc, yc, w, h = b\n",
        "        onehot = F.one_hot(torch.tensor(int(cls)), num_classes=num_classes)\n",
        "        feats.append(torch.cat([onehot.float(), torch.tensor([xc, yc, w, h])]))\n",
        "    if feats:\n",
        "        return torch.stack(feats)\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def encode_target_class11(boxes):\n",
        "    class11_boxes = boxes[boxes[:, 0] == 11][:, 1:]\n",
        "    if class11_boxes.shape[0] == 0:\n",
        "        return None\n",
        "    return class11_boxes.float()\n",
        "\n",
        "# === Loss functions ===\n",
        "\n",
        "def iou_loss(pred_boxes, target_boxes, eps=1e-6):\n",
        "    pred_xy1 = pred_boxes[:, :2] - pred_boxes[:, 2:] / 2\n",
        "    pred_xy2 = pred_boxes[:, :2] + pred_boxes[:, 2:] / 2\n",
        "    target_xy1 = target_boxes[:, :2] - target_boxes[:, 2:] / 2\n",
        "    target_xy2 = target_boxes[:, :2] + target_boxes[:, 2:] / 2\n",
        "\n",
        "    inter_min = torch.max(pred_xy1, target_xy1)\n",
        "    inter_max = torch.min(pred_xy2, target_xy2)\n",
        "    inter_wh = (inter_max - inter_min).clamp(min=0)\n",
        "    inter_area = inter_wh[:, 0] * inter_wh[:, 1]\n",
        "\n",
        "    pred_area = pred_boxes[:, 2] * pred_boxes[:, 3]\n",
        "    target_area = target_boxes[:, 2] * target_boxes[:, 3]\n",
        "\n",
        "    union_area = pred_area + target_area - inter_area + eps\n",
        "    iou = inter_area / union_area\n",
        "    return 1 - iou.mean()\n",
        "\n",
        "def combined_hungarian_loss(pred_boxes, target_boxes, l1_weight=10.0):\n",
        "    if target_boxes is None or pred_boxes.shape[0] == 0:\n",
        "        return torch.tensor(0., device=pred_boxes.device, requires_grad=True)\n",
        "    cost_matrix = torch.zeros(pred_boxes.shape[0], target_boxes.shape[0], device=pred_boxes.device)\n",
        "    for i, p in enumerate(pred_boxes):\n",
        "        for j, t in enumerate(target_boxes):\n",
        "            iou_comp = 1 - (1 - iou_loss(p.unsqueeze(0), t.unsqueeze(0)))  # IoU itself\n",
        "            l1_comp = F.l1_loss(p, t, reduction='mean')\n",
        "            cost_matrix[i, j] = (1 - iou_comp) + l1_weight * l1_comp\n",
        "    row_ind, col_ind = linear_sum_assignment(cost_matrix.detach().cpu().numpy())\n",
        "    matched_pred = pred_boxes[row_ind]\n",
        "    matched_target = target_boxes[col_ind].to(pred_boxes.device)\n",
        "    return iou_loss(matched_pred, matched_target) + l1_weight * F.l1_loss(matched_pred, matched_target)\n",
        "\n",
        "# === Positional Encoding ===\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n",
        "# === PaintingsTransformer Model ===\n",
        "\n",
        "class PaintingsTransformer(nn.Module):\n",
        "    def __init__(self, input_dim=16, d_model=64, nhead=4, num_layers=4, dropout=0.2, max_len=100):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Linear(input_dim, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.box_head = nn.Linear(d_model, 4)\n",
        "    def forward(self, x):\n",
        "        x = self.input_proj(x)\n",
        "        x = self.pos_enc(x)\n",
        "        x = self.encoder(x)\n",
        "        bbox_pred = self.box_head(x)\n",
        "        return bbox_pred\n",
        "\n",
        "# === Training/validation loop (with warmup, early stop, mixed precision) ===\n",
        "\n",
        "def get_warmup_scheduler(optimizer, warmup_steps):\n",
        "    def lr_lambda(current_step):\n",
        "        if current_step >= warmup_steps:\n",
        "            return 1.0\n",
        "        else:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "    return LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
        "\n",
        "def train_val_loop(model, train_loader, val_loader, epochs, lr, warmup_steps, device):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    warmup_scheduler = get_warmup_scheduler(optimizer, warmup_steps)\n",
        "    plateau_scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "    model.to(device)\n",
        "\n",
        "    scaler = GradScaler() if torch.cuda.is_available() else None\n",
        "    autocast_context = autocast if torch.cuda.is_available() else nullcontext\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    patience = 3\n",
        "    trigger_times = 0\n",
        "    global_step = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, Y in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
        "            optimizer.zero_grad()\n",
        "            batch_loss = 0\n",
        "            for xi, yi in zip(X, Y):\n",
        "                xi_enc = encode_input_objects(xi)\n",
        "                if xi_enc is None:\n",
        "                    continue\n",
        "                xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                yi_enc = encode_target_class11(yi)\n",
        "                if yi_enc is None:\n",
        "                    continue\n",
        "                yi_enc = yi_enc.to(device)\n",
        "                with autocast_context():\n",
        "                    bbox_pred = model(xi_enc).squeeze(0)\n",
        "                    loss = combined_hungarian_loss(bbox_pred, yi_enc)\n",
        "                if loss > 0:\n",
        "                    if scaler is not None:\n",
        "                        scaler.scale(loss).backward()\n",
        "                        scaler.step(optimizer)\n",
        "                        scaler.update()\n",
        "                    else:\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                    batch_loss += loss.item()\n",
        "                else:\n",
        "                    batch_loss += loss.item()\n",
        "                if global_step < warmup_steps:\n",
        "                    warmup_scheduler.step()\n",
        "                global_step += 1\n",
        "            train_loss += batch_loss\n",
        "        train_loss /= len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X, Y in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
        "                batch_loss = 0\n",
        "                for xi, yi in zip(X, Y):\n",
        "                    xi_enc = encode_input_objects(xi)\n",
        "                    if xi_enc is None:\n",
        "                        continue\n",
        "                    xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                    yi_enc = encode_target_class11(yi)\n",
        "                    if yi_enc is None:\n",
        "                        continue\n",
        "                    yi_enc = yi_enc.to(device)\n",
        "                    with autocast_context():\n",
        "                        bbox_pred = model(xi_enc).squeeze(0)\n",
        "                        loss = combined_hungarian_loss(bbox_pred, yi_enc)\n",
        "                    batch_loss += loss.item()\n",
        "                val_loss += batch_loss\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}: train_loss={train_loss:.6f}, val_loss={val_loss:.6f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            trigger_times = 0\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        if global_step >= warmup_steps:\n",
        "            plateau_scheduler.step(val_loss)\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "# === Hyperparameter grid search ===\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "def evaluate_val_loss(model, val_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for X, Y in val_loader:\n",
        "            for xi, yi in zip(X, Y):\n",
        "                xi_enc = encode_input_objects(xi)\n",
        "                if xi_enc is None:\n",
        "                    continue\n",
        "                xi_enc = xi_enc.unsqueeze(0).to(device)\n",
        "                yi_enc = encode_target_class11(yi)\n",
        "                if yi_enc is None:\n",
        "                    continue\n",
        "                yi_enc = yi_enc.to(device)\n",
        "                bbox_pred = model(xi_enc).squeeze(0)\n",
        "                loss = combined_hungarian_loss(bbox_pred, yi_enc)\n",
        "                total_loss += loss.item()\n",
        "                count += 1\n",
        "    return total_loss / max(count,1)\n",
        "\n",
        "def grid_search(param_grid, train_dataset, val_dataset, device, epochs=3, warmup_steps=100, log_csv_path=\"grid_search_log.csv\"):\n",
        "    keys = list(param_grid.keys())\n",
        "    best_val_loss = float('inf')\n",
        "    best_params = None\n",
        "    best_model_wts = None\n",
        "\n",
        "    def has_class_11(sample):\n",
        "        X, Y = sample\n",
        "        return (Y[:,0] == 11).any().item()\n",
        "\n",
        "    sample_weights = []\n",
        "    for i in range(len(train_dataset)):\n",
        "        sample = train_dataset[i]\n",
        "        sample_weights.append(5.0 if has_class_11(sample) else 1.0) # Oversample class 11 by 5x\n",
        "\n",
        "    sample_weights = torch.DoubleTensor(sample_weights)\n",
        "    train_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "    with open(log_csv_path, mode='w', newline='') as log_file:\n",
        "        writer = csv.writer(log_file)\n",
        "        writer.writerow(keys + [\"val_loss\"])\n",
        "\n",
        "        for values in itertools.product(*param_grid.values()):\n",
        "            params = dict(zip(keys, values))\n",
        "            print(f\"\\nTesting params: {params}\")\n",
        "\n",
        "            train_loader = DataLoader(train_dataset,\n",
        "                                      batch_size=params['batch_size'],\n",
        "                                      sampler=train_sampler,\n",
        "                                      num_workers=2,\n",
        "                                      pin_memory=torch.cuda.is_available(),\n",
        "                                      collate_fn=collate_fn)\n",
        "            val_loader = DataLoader(val_dataset,\n",
        "                                    batch_size=params['batch_size'],\n",
        "                                    shuffle=False,\n",
        "                                    num_workers=2,\n",
        "                                    pin_memory=torch.cuda.is_available(),\n",
        "                                    collate_fn=collate_fn)\n",
        "            model = PaintingsTransformer(input_dim=16, d_model=params['d_model'],\n",
        "                                         nhead=4, num_layers=params['num_layers'], dropout=0.2).to(device)\n",
        "            train_val_loop(model, train_loader, val_loader, epochs=epochs, lr=params['lr'], warmup_steps=warmup_steps, device=device)\n",
        "            val_loss = evaluate_val_loss(model, val_loader, device)\n",
        "            print(f\"Validation loss: {val_loss:.6f}\")\n",
        "            writer.writerow(list(values) + [val_loss])\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_params = params\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    print(\"\\nBest hyperparameters found:\", best_params)\n",
        "    torch.save(best_model_wts, \"best_model.pth\")\n",
        "    print(\"Best model saved as 'best_model.pth'\")\n",
        "    return best_params, best_model_wts\n",
        "\n",
        "# === Dataset and DataLoader Setup ===\n",
        "\n",
        "base_dir = \"/content/drive/My Drive/SmartSpace/homeobjects-3K/labels\"\n",
        "train_input_dir = f\"{base_dir}/train_input\"\n",
        "train_input_aug_dir = f\"{base_dir}/train_input_aug\"\n",
        "val_input_dir = f\"{base_dir}/val_input\"\n",
        "val_input_aug_dir = f\"{base_dir}/val_input_aug\"\n",
        "train_target_dir = f\"{base_dir}/train\"\n",
        "val_target_dir = f\"{base_dir}/val\"\n",
        "\n",
        "num_workers = 2\n",
        "pin_memory = torch.cuda.is_available()\n",
        "\n",
        "train_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(train_input_dir, train_target_dir),\n",
        "    FramePlacementDataset(train_input_aug_dir, train_target_dir)\n",
        "])\n",
        "val_dataset = ConcatDataset([\n",
        "    FramePlacementDataset(val_input_dir, val_target_dir),\n",
        "    FramePlacementDataset(val_input_aug_dir, val_target_dir)\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "param_grid = {\n",
        "    'lr': [1e-4, 1e-3],\n",
        "    'batch_size': [1, 4],\n",
        "    'num_layers': [3, 4],\n",
        "    'd_model': [64, 128]\n",
        "}\n",
        "\n",
        "# Run grid search to find best hyperparameters\n",
        "best_params, best_weights = grid_search(param_grid, train_dataset, val_dataset, device, epochs=5, warmup_steps=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "pmAF_vQDUCeV",
        "outputId": "d11226ec-d4ab-44e8-f502-5c031d30d4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing params: {'lr': 0.0001, 'batch_size': 1, 'num_layers': 4, 'd_model': 128}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1 [Train]: 100%|██████████| 2601/2601 [02:41<00:00, 16.11it/s]\n",
            "Epoch 1 [Val]: 100%|██████████| 477/477 [00:10<00:00, 46.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5: train_loss=1.878120, val_loss=1.414454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2 [Train]: 100%|██████████| 2601/2601 [02:39<00:00, 16.31it/s]\n",
            "Epoch 2 [Val]: 100%|██████████| 477/477 [00:09<00:00, 48.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/5: train_loss=1.678628, val_loss=1.345743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3 [Train]: 100%|██████████| 2601/2601 [02:36<00:00, 16.67it/s]\n",
            "Epoch 3 [Val]: 100%|██████████| 477/477 [00:10<00:00, 45.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/5: train_loss=1.597626, val_loss=1.169025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4 [Train]:  11%|█         | 288/2601 [00:16<02:15, 17.01it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2753752408.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;31m# Run grid search to find best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2753752408.py\u001b[0m in \u001b[0;36mgrid_search\u001b[0;34m(param_grid, train_dataset, val_dataset, device, epochs, warmup_steps, log_csv_path)\u001b[0m\n\u001b[1;32m    290\u001b[0m             model = PaintingsTransformer(input_dim=16, d_model=params['d_model'],\n\u001b[1;32m    291\u001b[0m                                          nhead=4, num_layers=params['num_layers'], dropout=0.2).to(device)\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mtrain_val_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarmup_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_val_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation loss: {val_loss:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2753752408.py\u001b[0m in \u001b[0;36mtrain_val_loop\u001b[0;34m(model, train_loader, val_loader, epochs, lr, warmup_steps, device)\u001b[0m\n\u001b[1;32m    176\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m                     \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                             )\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    950\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdevice_beta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# Nested if is necessary to bypass jitscript rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}